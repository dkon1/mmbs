

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification of linear models: eigenvalues and eigenvectors &#8212; Mathematical Methods for Biology</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Mathematical Methods for Biology</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  One-variable dynamical systems
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter1.html">
   Models with one variable in discrete time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter2.html">
   Discrete models of higher order: age-structured population models
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter4.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling-flow-in-the-phase-plane">
   Modeling: flow in the phase plane
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activators-and-inhibitors-in-biochemical-reactions">
     activators and inhibitors in biochemical reactions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#phase-plane-portraits">
     phase plane portraits
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-eigenvectors-and-eigenvalues">
   Analytical: eigenvectors and eigenvalues
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-linear-algebra-terminology">
     basic linear algebra terminology
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrices-transform-vectors">
     matrices transform vectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-eigenvalues">
     calculating eigenvalues
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculation-of-eigenvectors-on-paper">
     calculation of eigenvectors on paper
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-solutions-of-linear-2d-odes">
   Analytical: solutions of linear 2D ODEs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-classification-of-linear-systems">
   Computation: classification of linear systems
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#real-eigenvalues">
     real eigenvalues
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#complex-eigenvalues">
     complex eigenvalues
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-of-linear-systems">
     classification of linear systems
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthesis-dynamics-of-romantic-relationships">
   Synthesis: dynamics of romantic relationships
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="classification-of-linear-models-eigenvalues-and-eigenvectors">
<h1>Classification of linear models: eigenvalues and eigenvectors<a class="headerlink" href="#classification-of-linear-models-eigenvalues-and-eigenvectors" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In the last chapter we introduced and analyzed population models with
multiple variables representing different demographic groups. In those
models, the populations at the next time step depend on the population
at the current time step in a linear fashion. More generally, any model
with only linear dependencies can be represented in matrix form. In this
chapter we will learn how to analyze the behavior of these models, and
identify all possible classes of linear dynamical systems.</p>
<p>The main concept of this chapter are the special numbers and vectors
associated with a matrix, called eigenvalues and eigenvectors. Any
matrix can be thought of as an operator acting on vectors, and
transforming them in certain ways. Loosely speaking, this transformation
can be expressed in terms of special directions (eigenvectors) and
special numbers that describe what happens along those special
directions. Finding the eigenvalues and eigenvectors of a matrix allows
us to understand the dynamics of biological models by classifying them
into distinct categories.</p>
<p>In the modeling section, we will develop some intuition for modeling
activators and inhibitors of biochemical reactions. We will then learn
how to draw the flow of two-dimensional dynamical systems in the plane.
In the analytical section, we will define eigenvectors and eigenvalues,
and use this knowledge to find the general solution of linear
multi-variable systems. In the computational section, numerical
solutions of eigenvalues and eigenvectors will be applied to classifying
all linear multi-dimensional systems, and to plotting the solutions,
both over time and in the plane. Finally, in the synthesis section we
will use a light-hearted model of relationship dynamics to illustrate
how to analyze linear dynamical systems.</p>
</div>
<div class="section" id="modeling-flow-in-the-phase-plane">
<h2>Modeling: flow in the phase plane<a class="headerlink" href="#modeling-flow-in-the-phase-plane" title="Permalink to this headline">¶</a></h2>
<div class="section" id="activators-and-inhibitors-in-biochemical-reactions">
<h3>activators and inhibitors in biochemical reactions<a class="headerlink" href="#activators-and-inhibitors-in-biochemical-reactions" title="Permalink to this headline">¶</a></h3>
<p>Suppose two gene products (proteins) regulate each others’ expression.
Activator protein <span class="math notranslate nohighlight">\(A\)</span> binds to the promoter of the gene for <span class="math notranslate nohighlight">\(I\)</span> and
activates its expression, while inhibitor protein <span class="math notranslate nohighlight">\(I\)</span> binds to the
promoter of the gene for <span class="math notranslate nohighlight">\(A\)</span> and inhibits its expression (here the
variables stand for concentrations of the two proteins in the cell):
$<span class="math notranslate nohighlight">\(\begin{aligned}
\dot  A &amp; = &amp; - \alpha I\\
\dot  I &amp; = &amp; \beta A\end{aligned}\)</span><span class="math notranslate nohighlight">\( \)</span>\alpha<span class="math notranslate nohighlight">\( and \)</span>\beta<span class="math notranslate nohighlight">\( are positive
rate constants. They represent the rate of inhibition of \)</span>A<span class="math notranslate nohighlight">\( by \)</span>I<span class="math notranslate nohighlight">\(, and
of activation of \)</span>I<span class="math notranslate nohighlight">\( by \)</span>A<span class="math notranslate nohighlight">\(, respectively. Let us now complicate the
model by adding self-inhibition. It is common for regulatory proteins to
inhibit their own production. Then, we have the following system of
equations: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
\dot  A &amp; = &amp; - \gamma A - \alpha I\\
\dot  I &amp; = &amp; \beta A - \delta I\end{aligned}\)</span><span class="math notranslate nohighlight">\( Here we have added two
rates of self-inhibition \)</span>\gamma<span class="math notranslate nohighlight">\( and \)</span>\delta$. This is a system of two
coupled ODEs, and we will learn how to analyze these models both
analytically and graphically.</p>
</div>
<div class="section" id="phase-plane-portraits">
<h3>phase plane portraits<a class="headerlink" href="#phase-plane-portraits" title="Permalink to this headline">¶</a></h3>
<p>Before we learn about the analytical tools of linear algebra, let us
think intuitively about the effect of the variables on each other. The
best way to describe this is through plotting the geometry of the <em>flow</em>
prescribed by the differential equations. As we saw, for one-dimensional
ODEs the direction of the change of the dependent variable (also known
as the flow) could be shown as arrows on a line. A single variable can
only increase, decrease, or stay the same (at a fixed point). In two
dimensions there is more freedom. The flow is plotted on the <em>phase
plane</em>, where for any combination of the two variables (say <span class="math notranslate nohighlight">\(x,y\)</span>) the
ODE gives the derivatives of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This vector gives the flow, or
the rate of change at the particular point in the plane. Intuitively,
the flow describes the direction in which the system is pulling the
2-dimensional solution. If we plot the progress of a solution of ODE
(all the values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> starting with the initial condition) we
will obtain a <em>trajectory</em> in the phase plane. The arrows of the flow
are tangent to any trajectory curve, since they plot the derivatives of
<span class="math notranslate nohighlight">\(x,y\)</span>.</p>
<p><strong>Example: positive relationship between the variables</strong> Consider the
following system of differential equations: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot x &amp; = &amp; x + y \\
\dot y &amp; = &amp; x + 2y \nonumber
\label{eq:positive}\end{aligned}\)</span><span class="math notranslate nohighlight">\( This is system is coupled, with \)</span>x<span class="math notranslate nohighlight">\(
having an effect on \)</span>y<span class="math notranslate nohighlight">\( and vice versa. Specifically, the signs of the
constants mean that positive values of \)</span>x<span class="math notranslate nohighlight">\( have the effect of increasing
\)</span>y<span class="math notranslate nohighlight">\( (and vice versa), while negative values of \)</span>x<span class="math notranslate nohighlight">\( have the effect of
decreasing \)</span>y<span class="math notranslate nohighlight">\( (and vice versa). For any pair of values of \)</span>(x,y)<span class="math notranslate nohighlight">\(,
there is a flow prescribed by the ODEs. E.g., when \)</span>x=1, y=1<span class="math notranslate nohighlight">\(, the
derivatives are \)</span>\dot x = 2, \dot y = 3<span class="math notranslate nohighlight">\(. This means that the flow at
that point is given by the vector \)</span>(2,3)<span class="math notranslate nohighlight">\(, and can be plotted in the
\)</span>x,y<span class="math notranslate nohighlight">\( phase plane. This can be done for any pair of values of \)</span>x<span class="math notranslate nohighlight">\( and
\)</span>y$, and plotted to give the phase plane portrait, as in figure .</p>
<p><img alt="Phase plane flow for the system in equation []{data-label=&quot;fig:pp5&quot;}" src="images/week6_pp5.png" />{width=”3.5in”}</p>
<p>Observe that the overall dynamics of the systems are directed outward
from the origin, as we expect from the ODEs. The blue lines on the plot
are some sample trajectories. The solution over time for both <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(y\)</span> will either grow toward positive infinity, or decay to negative
infinity.</p>
<p><strong>Example: negative relationship between the variables</strong> Consider the
following system of differential equations, where <span class="math notranslate nohighlight">\(y\)</span> has an effect on
<span class="math notranslate nohighlight">\(\dot x\)</span> opposite of its own sign. That is, negative values of <span class="math notranslate nohighlight">\(y\)</span>
contribute to the growth of <span class="math notranslate nohighlight">\(x\)</span>, and vice versa. $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot x &amp; = &amp; -y \\
\dot y &amp; = &amp; x  \nonumber
\label{eq:negative}\end{aligned}\)</span><span class="math notranslate nohighlight">\( As above, the flow at any one point
is given by the ODEs. E.g. at \)</span>(0,1)<span class="math notranslate nohighlight">\( the two derivatives prescribe flow
in the \)</span>(1,0)<span class="math notranslate nohighlight">\( (up) direction, while at \)</span>(1,0)<span class="math notranslate nohighlight">\( the flow is in the
\)</span>(0,-1)$ direction. Figure shows the arrows of flow in the
phase plane around the origin. Note that the arrows go around in a
circular pattern around the origin.</p>
<p><img alt="Phase plane flow for the system in equation []{data-label=&quot;fig:pp2&quot;}" src="images/week6_pp2.png" />{width=”3.7in”}</p>
<p>Let us consider the trajectories of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> in time. The blue curves
in the phase plane plot demonstrate the solutions go around the origin
and return to the same point. This means that the behavior of the
solutions over time is <em>periodic</em>, with oscillations going from positive
to negative numbers and back forever.</p>
</div>
</div>
<div class="section" id="analytical-eigenvectors-and-eigenvalues">
<h2>Analytical: eigenvectors and eigenvalues<a class="headerlink" href="#analytical-eigenvectors-and-eigenvalues" title="Permalink to this headline">¶</a></h2>
<div class="section" id="basic-linear-algebra-terminology">
<h3>basic linear algebra terminology<a class="headerlink" href="#basic-linear-algebra-terminology" title="Permalink to this headline">¶</a></h3>
<p>We have seen matrix notation introduced in the previous chapter, along
with the definition of matrix multiplication. One basic advantage of
this notation is that it makes it possible to write any set of <em>linear
equations</em> as a single matrix equation. By linear equations we mean
those that contain only constants or first powers of the variables. The
field of mathematics studying matrices and their generalizations is
called <em>linear algebra</em>; it is fundamental to both pure and applied
mathematics. In this section we will learn some basic facts about
matrices and their properties. First of all, let us define some basic
terms:</p>
<ul class="simple">
<li><p>A matrix <span class="math notranslate nohighlight">\(A\)</span> is a rectangular array of <em>elements</em> <span class="math notranslate nohighlight">\(A_{ij}\)</span>, in which
<span class="math notranslate nohighlight">\(i\)</span> denotes the row number (index), counted from the top, and <span class="math notranslate nohighlight">\(j\)</span>
denotes the column number (index), counted from left to right.</p></li>
<li><p>The elements of a matrix <span class="math notranslate nohighlight">\(A\)</span> which have the same row and column
index, e.g. <span class="math notranslate nohighlight">\(A_{33}\)</span> are called the <em>diagonal elements</em>. Those which
do not lie on the diagonal are called the <em>off-diagonal</em> elements.
For instance, in the 3 x 3 matrix below, the elements <span class="math notranslate nohighlight">\(a, e, i\)</span> are
the diagonal elements:
$<span class="math notranslate nohighlight">\(A = \left(\begin{array}{ccc}a &amp; b &amp; c \\d &amp; e &amp; f \\g &amp; h &amp; i\end{array}\right)\)</span>$</p></li>
<li><p>The <em>trace</em> <span class="math notranslate nohighlight">\(\tau\)</span> of a matrix <span class="math notranslate nohighlight">\(A\)</span> is the sum of the diagonal
elements: <span class="math notranslate nohighlight">\(\tau = \sum_i A_{ii}\)</span></p></li>
<li><p>The <em>determinant</em> <span class="math notranslate nohighlight">\(\Delta\)</span> of a 2x2 matrix <span class="math notranslate nohighlight">\(A\)</span> is given by the
following: <span class="math notranslate nohighlight">\(\Delta = ad - bc\)</span>, where
$<span class="math notranslate nohighlight">\(A = \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\)</span>$ For
larger matrices, the determinant is defined recursively, in terms of
2x2 submatrices of the larger matrix, but we will not give the full
definition here.</p></li>
</ul>
<p>In chapter 3 you learned the rule of matrix multiplication, and we can
write <span class="math notranslate nohighlight">\(C = A \times B\)</span>, so long as the number of columns in <span class="math notranslate nohighlight">\(A\)</span> matches
the number of rows in <span class="math notranslate nohighlight">\(B\)</span>. However, what if we want to reverse the
process? If we know the resulting matrix <span class="math notranslate nohighlight">\(C\)</span>, and one of the two
matrices, e.g. <span class="math notranslate nohighlight">\(A\)</span>, how can we find <span class="math notranslate nohighlight">\(B\)</span>? Naively, we would like to be
able to divide both sides by the matrix <span class="math notranslate nohighlight">\(A\)</span>, and find <span class="math notranslate nohighlight">\(B = C/A\)</span>.
However, things are more complicated for matrices.</p>
<p>Properly speaking, we need to introduce the <em>inverse</em> of a matrix <span class="math notranslate nohighlight">\(A\)</span>.
If we think about inverses of real numbers, <span class="math notranslate nohighlight">\(a^{-1}\)</span> is a number that
when it multiplies <span class="math notranslate nohighlight">\(a\)</span>, results in one. In order to define the
equivalent for matrices, we first need to introduce the unity of matrix
multiplication.</p>
<p>The <em>identity matrix</em> is an N x N square matrix in which all the
diagonal elements are 1 and all the off-diagonal elements are zero, as
shown here:
$<span class="math notranslate nohighlight">\(I = \left(\begin{array}{ccccc}1 &amp; 0 &amp; ... &amp; 0 &amp; 0 \\0 &amp; 1 &amp; ... &amp; ... &amp; 0 \\... &amp; ... &amp; ... &amp; ... &amp; ... \\0 &amp; ... &amp; ... &amp; 1 &amp; 0 \\0 &amp; 0 &amp; ... &amp; 0 &amp; 1\end{array}\right)\)</span>$</p>
<p>The main property of the identity matrix is that when multiplied by any
matrix of matching size, the result is unchanged: $<span class="math notranslate nohighlight">\(AI = IA = A\)</span>$ You
should be able to convince yourself that this is true for any 2x2 matrix
and the identity. Now that we have a matrix unity, we can define the
inverse of a matrix:</p>
<p>A square matrix <span class="math notranslate nohighlight">\(A\)</span> has an <em>inverse matrix</em> <span class="math notranslate nohighlight">\(A^{-1}\)</span> if it satisfies the
following: $<span class="math notranslate nohighlight">\(A^{-1} A = A A^{-1} = I\)</span>$</p>
<p>Finding the inverse of a matrix is not simple, and we will be content to
let computers handle the dirty work. In fact, not every matrix possesses
an inverse. There is a test for existence of an inverse of <span class="math notranslate nohighlight">\(A\)</span>, and it
depends on the determinant [&#64;strang_linear_2005]:</p>
<p>A square matrix <span class="math notranslate nohighlight">\(A\)</span> possesses an inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span> and is called
<em>invertible</em> if and only if its determinant is not zero.</p>
</div>
<div class="section" id="matrices-transform-vectors">
<h3>matrices transform vectors<a class="headerlink" href="#matrices-transform-vectors" title="Permalink to this headline">¶</a></h3>
<p>In this section we will learn to characterize square matrices by finding
special numbers and vectors associated with them. At the core of this
analysis lies the concept of a matrix as an <em>operator</em> that transforms
vectors by multiplication. To be clear, in this section we take as
default that the matrices <span class="math notranslate nohighlight">\(A\)</span> are square, and that vectors <span class="math notranslate nohighlight">\(\vec v\)</span> are
column vectors, and thus will multiply the matrix on the right:
<span class="math notranslate nohighlight">\(A \times  \vec v\)</span>.</p>
<p>A matrix multiplied by a vector produces another vector, provided the
number of columns in the matrix is the same as the number of rows in the
vector. This can be interpreted as the matrix transforming the vector
<span class="math notranslate nohighlight">\(\vec v\)</span> into another one: <span class="math notranslate nohighlight">\( A  \times  \vec v = \vec u\)</span>. The resultant
vector <span class="math notranslate nohighlight">\(\vec u\)</span> may or may not resemble <span class="math notranslate nohighlight">\(\vec v\)</span>, but there are special
vectors for which the transformation is very simple.</p>
<p><strong>Example.</strong> Let us multiply the following matrix and vector (specially
chosen to make a point):
$<span class="math notranslate nohighlight">\(\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ -1 \end{array}\right) = \left(\begin{array}{c}2 -1 \\ 2 - 3 \end{array}\right) =  \left(\begin{array}{c} 1 \\ -1 \end{array}\right)\)</span><span class="math notranslate nohighlight">\(
We see that this particular vector is unchanged when multiplied by this
matrix, or we can say that the matrix multiplication is equivalent to
multiplication by 1. Here is another such vector for the same matrix:
\)</span><span class="math notranslate nohighlight">\(\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ 2 \end{array}\right) = \left(\begin{array}{c}2 +2 \\ 2 + 6 \end{array}\right) =  \left(\begin{array}{c} 4 \\ 8 \end{array}\right)\)</span>$
In this case, the vector is changed, but only by multiplication by a
constant (4). Thus the geometric direction of the vector remained
unchanged.</p>
<p>Generally, a square matrix has an associated set of vectors for which
multiplication by the matrix is equivalent to multiplication by a
constant. This can be written down as a definition:</p>
<p>An <em>eigenvector</em> of a square matrix <span class="math notranslate nohighlight">\(A\)</span> is a vector <span class="math notranslate nohighlight">\(\vec v\)</span> for which
matrix multiplication by <span class="math notranslate nohighlight">\(A\)</span> is equivalent to multiplication by a
constant. This constant <span class="math notranslate nohighlight">\(\lambda\)</span> is called its <em>eigenvalue</em> of <span class="math notranslate nohighlight">\(A\)</span>
corresponding the the eigenvector <span class="math notranslate nohighlight">\(\vec v\)</span>. The relationship is
summarized in the following equation:
$<span class="math notranslate nohighlight">\(A  \times  \vec v = \lambda \vec v\)</span>$ [def:eigen]</p>
<p>Note that this equation combines a matrix (<span class="math notranslate nohighlight">\(A\)</span>), a vector (<span class="math notranslate nohighlight">\(\vec v\)</span>) and
a scalar <span class="math notranslate nohighlight">\(\lambda\)</span>, and that both sides of the equation are column
vectors. This definition is illustrated in figure , showing a vector (<span class="math notranslate nohighlight">\(v\)</span>) multiplied by a matrix
<span class="math notranslate nohighlight">\(A\)</span>, and the resulting vector <span class="math notranslate nohighlight">\(\lambda v\)</span>, which is in the same
direction as <span class="math notranslate nohighlight">\(v\)</span>, due to scalar multiplying all elements of a vector,
thus either stretching it if <span class="math notranslate nohighlight">\(\lambda&gt;1\)</span> or compressing it if
<span class="math notranslate nohighlight">\(\lambda &lt; 1\)</span>. This assumes that <span class="math notranslate nohighlight">\(\lambda\)</span> is a real number, which is
not always the case, but we will leave that complication aside for the
purposes of this chapter.</p>
<p><img alt="Illustration of the geometry of a matrix  multiplying itseigenvector , resulting in a vector in the same direction. Figure by Lantonov under CC BY-SA 4.0 via WikimediaCommons.[]{data-label=&quot;fig:ch13_eigenvector&quot;}" src="images/Eigenvalue_equation.png" />{width=”3in”}</p>
<p>The definition does not specify how many such eigenvectors and
eigenvalues can exist for a given matrix <span class="math notranslate nohighlight">\(A\)</span>. There are usually as many
such vectors <span class="math notranslate nohighlight">\(\vec v\)</span> and corresponding numbers <span class="math notranslate nohighlight">\(\lambda\)</span> as the number
of rows or columns of the square matrix <span class="math notranslate nohighlight">\(A\)</span>, so a 2 by 2 matrix has two
eigenvectors and two eigenvalues, a 5x5 matrix has 5 of each, etc. One
ironclad rule is that there cannot be more distinct eigenvalues than the
matrix dimension. Some matrices possess fewer eigenvalues than the
matrix dimension, those are said to have a <em>degenerate</em> set of
eigenvalues, and at least two of the eigenvectors share the same
eigenvalue.</p>
<p>The situation with eigenvectors is trickier. There are some matrices for
which any vector is an eigenvector, and others which have a limited set
of eigenvectors. What is difficult about counting eigenvectors is that
an eigenvector is still an eigenvector when multiplied by a constant.
You can show that for any matrix, multiplication by a constant is
commutative: <span class="math notranslate nohighlight">\(cA = Ac \)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is a matrix and <span class="math notranslate nohighlight">\(c\)</span> is a constant.
This leads us to the important result that if <span class="math notranslate nohighlight">\(\vec v\)</span> is an eigenvector
with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, then any scalar multiple <span class="math notranslate nohighlight">\(c \vec v\)</span> is also
an eigenvector with the same eigenvalue. The following demonstrates this
algebraically:
$<span class="math notranslate nohighlight">\(A  \times  (c \vec v) = c A  \times  \vec v = c \lambda \vec v =  \lambda (c \vec v)\)</span><span class="math notranslate nohighlight">\(
This shows that when the vector \)</span>c \vec v<span class="math notranslate nohighlight">\( is multiplied by the matrix
\)</span>A<span class="math notranslate nohighlight">\(, it results in its being multiplied by the same number \)</span>\lambda<span class="math notranslate nohighlight">\(, so
by definition it is an eigenvector. Therefore, an eigenvector \)</span>\vec v<span class="math notranslate nohighlight">\(
is not unique, as any constant multiple \)</span>c \vec v<span class="math notranslate nohighlight">\( is also an
eigenvector. It is more useful to think not of a single eigenvector
\)</span>\vec v<span class="math notranslate nohighlight">\(, but of a **collection of vectors that can be interconverted by
scalar multiplication** that are all essentially the same eigenvector.
Another way to represent this, if the eigenvector is real, is that an
eigenvector as a **direction that remains unchanged by multiplication by
the matrix**, such as direction of the vector \)</span>v$ in figure . As mentioned above, this is true only for
real eigenvalues and eigenvectors, since complex eigenvectors cannot be
used to define a direction in a real space.</p>
<p>To summarize, eigenvalues and eigenvectors of a matrix are a set of
numbers and a set of vectors (up to scalar multiple) that describe the
action of the matrix as a multiplicative operator on vectors.
“Well-behaved” square <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrices have <span class="math notranslate nohighlight">\(n\)</span> distinct eigenvalues
and <span class="math notranslate nohighlight">\(n\)</span> eigenvectors pointing in distinct directions. In a deep sense,
the collection of eigenvectors and eigenvalues defines a matrix <span class="math notranslate nohighlight">\(A\)</span>,
which is why an older name for them is characteristic vectors and
values.</p>
</div>
<div class="section" id="calculating-eigenvalues">
<h3>calculating eigenvalues<a class="headerlink" href="#calculating-eigenvalues" title="Permalink to this headline">¶</a></h3>
<p>Finding the eigenvalues and eigenvectors analytically, that is on paper,
is quite laborious even for 3 by 3 or 4 by 4 matrices and for larger
ones there is no analytical solution. In practice, the task is
outsourced to a computer, and MATLAB has a number of functions for this
purpose. Nevertheless, it is useful to go through the process in 2
dimensions in order to gain an understanding of what is involved. From
the definition [def:eigen] of eigenvalues and eigenvectors, the
condition can be written in terms of the four elements of a 2 by 2
matrix:
$<span class="math notranslate nohighlight">\(\left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)\)</span><span class="math notranslate nohighlight">\(
This is now a system of two linear algebraic equations, which we can
solve by substitution. First, let us solve for \)</span>v_1<span class="math notranslate nohighlight">\( in the first row,
to get \)</span><span class="math notranslate nohighlight">\(v_1 = \frac{-bv_2}{a-\lambda}\)</span><span class="math notranslate nohighlight">\( Then we substitute this into
the second equation and get:
\)</span><span class="math notranslate nohighlight">\(\frac{-bcv_2}{a-\lambda} +(d-\lambda)v_2 = 0\)</span><span class="math notranslate nohighlight">\( Since \)</span>v_2<span class="math notranslate nohighlight">\( multiplies
both terms, and is not necessarily zero, we require that its
multiplicative factor be zero. Doing a little algebra, we obtain the
following, known as the *characteristic equation* of the matrix:
\)</span><span class="math notranslate nohighlight">\(-bc +(a-\lambda)(d-\lambda) = \lambda^2-(a+d)\lambda +ad-bc = 0\)</span><span class="math notranslate nohighlight">\( This
equation can be simplified by using two quantities we defined at the
beginning of the section: the sum of the diagonal elements called the
trace \)</span>\tau = a+d<span class="math notranslate nohighlight">\(, and the determinant \)</span>\Delta = ad-bc<span class="math notranslate nohighlight">\(. The quadratic
equation has two solutions, dependent solely on \)</span>\tau<span class="math notranslate nohighlight">\( and \)</span>\Delta<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\lambda = \frac{\tau \pm \sqrt{\tau^2-4\Delta}}{2}
\label{eq:2D_eig}\)</span><span class="math notranslate nohighlight">\( This is the general expression for a 2 by 2 matrix,
showing there are two possible eigenvalues. Note that if
\)</span>\tau^2-4\Delta&gt;0<span class="math notranslate nohighlight">\(, the eigenvalues are real, if \)</span>\tau^2-4\Delta&lt;0<span class="math notranslate nohighlight">\(,
they are complex (have real and imaginary parts), and if
\)</span>\tau^2-4\Delta=0$, there is only one eigenvalue. This situation is
known as degenerate, because two eigenvectors share the same eigenvalue.</p>
<p><strong>Example.</strong> Let us take the same matrix we looked at in the previous
subsection:
$<span class="math notranslate nohighlight">\(A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\)</span><span class="math notranslate nohighlight">\( The trace
of this matrix is \)</span>\tau = 2+3 =5<span class="math notranslate nohighlight">\( and the determinant is
\)</span>\Delta = 6 - 2 = 4<span class="math notranslate nohighlight">\(. Then by our formula, the eigenvalues are:
\)</span><span class="math notranslate nohighlight">\(\lambda = \frac{5 \pm \sqrt{5^2-4 \times 4}}{2}  =  \frac{5 \pm 3}{2}  = 4, 1\)</span>$
These are the multiples we found in the example above, as expected.</p>
<p>A real matrix can have complex eigenvalues and eigenvectors, but
whenever it acts on a real vector, the result is still real. This is
because the complex numbers cancel each other’s imaginary parts. For
discrete time models, it is enough to consider the absolute value of a
complex eigenvalue, which is defined as following:
<span class="math notranslate nohighlight">\(|a +b i |= \sqrt{a^2 + b^2}\)</span>. As before, the eigenvalue with the
largest absolute value “wins” in the long term.</p>
</div>
<div class="section" id="calculation-of-eigenvectors-on-paper">
<h3>calculation of eigenvectors on paper<a class="headerlink" href="#calculation-of-eigenvectors-on-paper" title="Permalink to this headline">¶</a></h3>
<p>The surprising fact is that, as we saw in the last subsection, the
eigenvalues of a matrix can be found without knowing its eigenvectors!
However, the converse is not true: to find the eigenvectors, one first
needs to know the eigenvalues. Given an eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, let us
again write down the defining equation of the eigenvector for a generic
2 by 2 matrix:
$<span class="math notranslate nohighlight">\(\left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)\)</span><span class="math notranslate nohighlight">\(
This vector equation is equivalent to two algebraic equations:
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
av_1 + b v_2 &amp;= \lambda v_1 \\
cv_1 + d v_2 &amp;= \lambda v_2 \end{aligned}\)</span><span class="math notranslate nohighlight">\( Since we’ve already found
\)</span>\lambda<span class="math notranslate nohighlight">\( by solving the characteristic equation, this is two linear
equations with two unknowns (\)</span>v_1<span class="math notranslate nohighlight">\( and \)</span>v_2<span class="math notranslate nohighlight">\(). You may remember from
advanced algebra that such equations may either have a single solution
for each unknown, but sometimes they may have none, or infinitely many
solutions. Since there are unknowns on both sides of the equation, we
can make both equations be equal to zero: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
(a-\lambda)v_1 + b v_2 &amp;= 0 \\
cv_1 + (d-\lambda ) v_2 &amp;=0\end{aligned}\)</span><span class="math notranslate nohighlight">\( So the first equation yields
the relationship \)</span>v_1 = -v_2 b/(a-\lambda) <span class="math notranslate nohighlight">\( and the second equation is
\)</span>v_1 = -v_2(d-\lambda)/c<span class="math notranslate nohighlight">\(, which we already obtained in the last
subsection. We know that these two equations must be the same, since the
ratio of \)</span>v_1<span class="math notranslate nohighlight">\( and \)</span>v_2$ is what defines the eigenvector. So we can use
either expression to find the eigenvector.</p>
<p><strong>Example.</strong> Let us return to the same matrix we looked at in the
previous subsection:
$<span class="math notranslate nohighlight">\(A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\)</span><span class="math notranslate nohighlight">\( The
eigenvalues of the matrix are 1 and 4. Using our expression above, where
the element \)</span>a=2<span class="math notranslate nohighlight">\( and \)</span>b=1<span class="math notranslate nohighlight">\(, let us find the eigenvector corresponding
to the eigenvalue 1: \)</span><span class="math notranslate nohighlight">\(v_1 = - v_2 \times  1/(2-1) = - v_2\)</span><span class="math notranslate nohighlight">\( Therefore
the eigenvector is characterized by the first and second elements being
negatives of each other. We already saw in the example two subsections
above that the vector \)</span>(1,-1)<span class="math notranslate nohighlight">\( is such as eigenvector, but it is also
true of the vectors \)</span>(-1,1)<span class="math notranslate nohighlight">\(, \)</span>(-\pi, \pi)<span class="math notranslate nohighlight">\( and \)</span>(10^6, -10^6)$. This
infinite collection of vectors, all along the same direction, can be
described as the eigenvector (or eigendirection) corresponding to the
eigenvalue 1.</p>
<p>Repeating this procedure for <span class="math notranslate nohighlight">\(\lambda = 4\)</span>, we obtain the linear
relationship: $<span class="math notranslate nohighlight">\(v_1 = - v_2 \times  1/(2-4) = 0.5 v_2\)</span><span class="math notranslate nohighlight">\( Once again, the
example vector we saw two subsections \)</span>(2,1)<span class="math notranslate nohighlight">\( is in agreement with our
calculation. Other vectors that satisfy this relationship include
\)</span>(10,5)<span class="math notranslate nohighlight">\(, \)</span>(-20,-10)<span class="math notranslate nohighlight">\(, and \)</span>(-0.4,-0.2)$. This is again a collection of
vectors that are all considered the same eigenvector with eigenvalue 4
which are all pointing in the same direction, with the only difference
being their length.</p>
</div>
</div>
<div class="section" id="analytical-solutions-of-linear-2d-odes">
<h2>Analytical: solutions of linear 2D ODEs<a class="headerlink" href="#analytical-solutions-of-linear-2d-odes" title="Permalink to this headline">¶</a></h2>
<p>Let us start by considering two variable ODEs that do not affect each
other: <strong>Example: two uncoupled ODEs</strong> In general, for a two-variable
system, the value of one variable affects the other. In the equations
above, the terms with the constants <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> provide what is known as
<em>coupling</em> between the two variables. Let us look at the primitive
situation where the two variables are uncoupled, as an illustration of
solving two-dimensional ODEs. If we set the coupling constants <span class="math notranslate nohighlight">\(b\)</span> and
<span class="math notranslate nohighlight">\(c\)</span> to 0, we get: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot x &amp; = &amp; ax \\
\dot y &amp; = &amp; dy\end{aligned}\)</span><span class="math notranslate nohighlight">\( Using our knowledge of 1D linear ODE, we
can solve the two equations independently to get the following:
\)</span>x(t) = x_0 e^{at}<span class="math notranslate nohighlight">\( and \)</span>y(t) = y_0 e^{dt}<span class="math notranslate nohighlight">\(. The solutions can be
written in vector form:
\)</span><span class="math notranslate nohighlight">\(\left(\begin{array}{c}x(t) \\y(t)\end{array}\right) = x_0 e^{at} \left(\begin{array}{c}1 \\0\end{array}\right)+y_0 e^{dt}\left(\begin{array}{c}0\\1\end{array}\right)\)</span><span class="math notranslate nohighlight">\(
This is another way of writing that the dynamics of variable \)</span>x<span class="math notranslate nohighlight">\( is
exponential growth (or decay) with rate \)</span>a<span class="math notranslate nohighlight">\(, and ditto \)</span>y<span class="math notranslate nohighlight">\(, with rate
\)</span>d<span class="math notranslate nohighlight">\(. Given the initial conditions \)</span>(x_0, y_0)$, we can divide the
behavior of the solutions into a sum of two vectors, each growing or
decaying at its own rate.</p>
<p>Linear algebra allows us to find the solution for two-dimensional ODEs
where the variables are interdependent using the same idea. The general
(homogeneous) ODE with two dependent variables can be written as
follows: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot x &amp; = &amp; ax + by \\
\dot y &amp; = &amp; cx + dy\end{aligned}\)</span><span class="math notranslate nohighlight">\( We can write this in matrix form
like this:
\)</span><span class="math notranslate nohighlight">\(\left(\begin{array}{c}\dot x \\ \dot y \end{array}\right) = \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}x \\ y \end{array}\right)\)</span>$</p>
<p>Let us call the matrix <span class="math notranslate nohighlight">\(A\)</span>, and represent the vector <span class="math notranslate nohighlight">\((x,y)\)</span> as
<span class="math notranslate nohighlight">\(\vec {x}\)</span>, then the general linear equation can be written like this:
$<span class="math notranslate nohighlight">\(\dot{ \vec{ x}} = A \vec {x }\)</span><span class="math notranslate nohighlight">\( This notation is intended to make
plain the similarity with the linear 1D ODE: \)</span>\dot x = a x<span class="math notranslate nohighlight">\(. This
similarity is deep and substantial, in that linear equations in multiple
dimensions share the same basic exponential form. In general, all
solutions of linear equations can be written as a sum of exponentials
multiplying different vectors, so in 2D we have:
\)</span><span class="math notranslate nohighlight">\(\left(\begin{array}{c} x(t) \\  y(t) \end{array}\right) =C_1e^{\lambda_1 t} \left(\begin{array}{c}x_1\\y_1\end{array}\right)+C_2 e^{\lambda_2 t}\left(\begin{array}{c}x_2\\y_2\end{array}\right)\)</span><span class="math notranslate nohighlight">\(
The constants \)</span>C_1, C_2<span class="math notranslate nohighlight">\( are determined by the initial conditions, while
the constants \)</span>\lambda_1, \lambda_2<span class="math notranslate nohighlight">\( are the eigenvalues and the vectors
\)</span>(x_1,y_1)<span class="math notranslate nohighlight">\( and \)</span>(x_2,y_2)<span class="math notranslate nohighlight">\( are the eigenvectors of the matrix \)</span>A$. We
will now consider the application of this general result using
computational tools.</p>
</div>
<div class="section" id="computation-classification-of-linear-systems">
<h2>Computation: classification of linear systems<a class="headerlink" href="#computation-classification-of-linear-systems" title="Permalink to this headline">¶</a></h2>
<p>We have seen that linear algebra allows us to write down the solution of
a multivariable dynamical system into a sum of exponential terms. In
this section we use computational techniques to find the eigenvalues and
eigenvectors of a system, and then produce the <em>phase portraits</em> of the
linear systems. There are only a few different types of flow possible
for linear systems, and we will classify them.</p>
<p><img alt="Phase plane flow for a linear system with a saddlepoint[]{data-label=&quot;fig:pp1&quot;}" src="images/week6_pp1.png" />{width=”3.7in”}</p>
<div class="section" id="real-eigenvalues">
<h3>real eigenvalues<a class="headerlink" href="#real-eigenvalues" title="Permalink to this headline">¶</a></h3>
<p>Let us consider the fixed points of the linear system: since both
<span class="math notranslate nohighlight">\(\dot x =0 \)</span> and <span class="math notranslate nohighlight">\(\dot y = 0\)</span> must be zero, the only fixed point is the
origin <span class="math notranslate nohighlight">\((0,0)\)</span>. We will see that the stability of the fixed point
depends on the sign of the real part of the eigenvalue.</p>
<p>Suppose we have a positive real eigenvalue. The solution in the
direction of the corresponding eigenvector is then described by
<span class="math notranslate nohighlight">\(Ce^{\lambda t}\)</span>, <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>, which is exponential growth. The means
that the solution is going to grow in the direction of the eigenvector
away from the origin, and thus the origin is an unstable fixed point (in
this direction). On the other hand, if <span class="math notranslate nohighlight">\(\lambda &lt; 0\)</span>, the solution
decays exponentially and thus approaches the origin, so the fixed point
is stable (in this direction).</p>
<p>There are two different eigenvalues, and one may be positive while
another is negative. In this case, the fixed point is is called a
<em>saddle point</em> for geometric reasons: solutions flow toward it in one
direction, like a marble along the forward-backward axis of a saddle on
a horse and flow away from it along the sideways direction on a saddle.
Then, the fixed point is stable when approached along one eigenvector,
but unstable along the other. What happens if the initial condition is
not on either eigenvector? I will use a fact of linear algebra that
given any two (non-colinear) 2D vectors, any vector in the plane can
represented as a sum (with some coefficients) of these two. Thus, the
general solution can be written as follows:
$<span class="math notranslate nohighlight">\(\left(\begin{array}{c} x(t) \\  y(t) \end{array}\right) =C_1e^{at} \left(\begin{array}{c}v_1\\v_2\end{array}\right)+C_2 e^{-bt}\left(\begin{array}{c}u_1\\u_2\end{array}\right)\)</span><span class="math notranslate nohighlight">\(
where \)</span>a,b&gt;0<span class="math notranslate nohighlight">\(. Then we see that the component in the direction of the
first eigenvector will grow, while the component along the second
eigenvector will decay. Thus, as \)</span>t \rightarrow \infty$, all solutions
will approach the vector with the unstable eigenvalue, except those with
initial conditions right on the eigenvector corresponding to the stable
eigenvalue. This means that the fixed point is essentially unstable,
because only trajectories which start exactly along the stable direction
approach the fixed point in the long run, while others, may approach the
fixed point for a finite time, flow away when the unstable component
with the positive eigenvalue takes over, as shown in figure .</p>
<p><img alt="Exponentially decaying oscillations: time plot of , and phase planeof both  and[]{data-label=&quot;fig:exp_osc&quot;}" src="images/lec7_exp_osc.png" />{width=”3.4in”}
<img alt="Exponentially decaying oscillations: time plot of , and phase planeof both  and[]{data-label=&quot;fig:exp_osc&quot;}" src="images/lec7_pp1.png" />{width=”2.6in”}</p>
</div>
<div class="section" id="complex-eigenvalues">
<h3>complex eigenvalues<a class="headerlink" href="#complex-eigenvalues" title="Permalink to this headline">¶</a></h3>
<p>If the argument of the square root is negative, eigenvalues may be
complex numbers, which we can write like this: <span class="math notranslate nohighlight">\(a+bi\)</span>. Using Euler’s
formula, we can write down the time-dependent part of the solutions as
the following:
$<span class="math notranslate nohighlight">\(e^{(a + bi)t} = e^{at}e^{bit}= e^{at}(\cos(bt)+i\sin(bt))\)</span>$ The
behavior of these solutions combines exponential growth or decay from
the real part, with the oscillations produced by the imaginary part.
This describes either exponentially growing or decaying oscillations,
which look like decaying waves in time, or as a spiral in the phase
plane:</p>
<p>Thus we see that the stability of the fixed point with complex
eigenvalues depends on the sign of the real part. Purely imaginary
eigenvalues produce periodic oscillations, which keep the same
amplitude, as we saw in the example in the modeling section.</p>
</div>
<div class="section" id="classification-of-linear-systems">
<h3>classification of linear systems<a class="headerlink" href="#classification-of-linear-systems" title="Permalink to this headline">¶</a></h3>
<p>Stability:   <span class="math notranslate nohighlight">\(Re(\lambda_1, \lambda_2)&gt;0\)</span>   <span class="math notranslate nohighlight">\(Re(\lambda_1, \lambda_2)&lt;0\)</span>   <span class="math notranslate nohighlight">\(Re(\lambda_1)&lt;0 , Re(\lambda_2)&gt;0\)</span>   <span class="math notranslate nohighlight">\(Re(\lambda_1\; or \; \lambda_2)= 0 \)</span></p>
<hr class="docutils" />
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> real:             unstable node                   stable node                       saddle point                            fixed line
complex:          unstable spiral                 stable spiral                           N/A                               center point
</pre></div>
</div>
<p>: Eigenvalues of linear ODEs define type of phase plane</p>
<p>[default]</p>
<p>The table above summarizes all the different types of flows in the phase
plane possible for linear systems, in terms of the behavior of solutions
relative to the fixed point at the origin. If the eigenvalues are real,
the solutions will be exponential in nature. There are three
possibilities for nonzero eigenvalues: <em>stable node</em> (both eigenvalues
are negative), <em>unstable node</em> (both eigenvalues are positive), and a
<em>saddle point</em> (mixed signs). If one of the eigenvalues is zero, this
means that there is not flow along one direction, so there is a <em>line of
fixed points</em> in the direction of the corresponding eigenvector (if both
eigenvalues are zero, there is no flow at all.)</p>
<p>For complex eigenvalues, there are three possibilities: if the real part
is positive, the solution will grow and oscillate (oscillations with
exponentially increasing amplitude), if the real part is negative, the
solution will decay and oscillation (oscillations with exponentially
increasing amplitude), and if the real part is zero (pure imaginary
eigenvalues) the solution will oscillate with constant amplitude. The
first type is called an <em>unstable spiral</em>, the second a <em>stable spiral</em>,
and the third a <em>center</em>. It is not possible for complex eigenvalues of
two-dimensional systems to have different signs of real parts, because
as the formula shows, the real part is the same for both and is equal to
the trace divided by two.</p>
</div>
</div>
<div class="section" id="synthesis-dynamics-of-romantic-relationships">
<h2>Synthesis: dynamics of romantic relationships<a class="headerlink" href="#synthesis-dynamics-of-romantic-relationships" title="Permalink to this headline">¶</a></h2>
<p>We examine a model, taken from [&#64;strogatz_nonlinear_2001], that applies
dynamical systems modeling to a pressing concern for many humans: the
prediction of dynamics of a romantic relationship. There are several
unrealistic assumptions involved in the following model: first, that
love or affection can be quantified, second, that any changes in
relationship depend only on the emotions of the two people involved, and
third, that the rate of change of the two love variables depend linearly
on each other.</p>
<p>If we can give those assumptions the benefit of the doubt (which is how
all relationships begin), we can write down a system of ODEs to describe
a romantically involved couple. Here <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dynamic variables
that quantify the emotional states of the two lovers: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot  X &amp; = &amp; aX+ bY \\
\dot  Y &amp; = &amp; cX + dY\end{aligned}\)</span><span class="math notranslate nohighlight">\( Let us denote positive feelings
(love) with positive values of \)</span>X,Y<span class="math notranslate nohighlight">\(, while negative values signify
negative feelings (hate.) The significance of the parameters can be
interpreted as follows: \)</span>a,d<span class="math notranslate nohighlight">\( describe the response of the two people to
their own feelings, while \)</span>b,c<span class="math notranslate nohighlight">\( correspond to the effect the other
person’s feeling has on their own. For example, a person whose feeling
grow as the other person’s affection increases can be modeled with a
positive value of \)</span>b<span class="math notranslate nohighlight">\( (or \)</span>c<span class="math notranslate nohighlight">\(). On the other hand, a person whose own
feelings are dampened by the other one’s excessively positive emotions,
can be decribed by a negative value of \)</span>b<span class="math notranslate nohighlight">\( or \)</span>c<span class="math notranslate nohighlight">\(. Their own feelings
can also play a role, either positive or negative, reflected in the sign
of the constants \)</span>a<span class="math notranslate nohighlight">\( and \)</span>d$.</p>
<p>Using mathematical modeling, we can answer the following basic
questions:</p>
<ol class="simple">
<li><p>Given a set of values for parameters <span class="math notranslate nohighlight">\(a, b, c, d\)</span>, predict the
dynamic behavior of the model relationship.</p></li>
<li><p>Find conditions for stability and existence of oscillations in the
dynamical system, expressed as a function of the parameters.</p></li>
</ol>
<p>To address the first question, here are some simplified scenarios for
our two lovers in the model.</p>
<p><strong>Detached lovers:</strong> Let the emotional state of the two lovers depend
only on their own emotions, for example: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot  X &amp; = &amp; X \\
\dot  Y &amp; = &amp;  -Y\end{aligned}\)</span><span class="math notranslate nohighlight">\( To classify the behavior of the model,
we find the eigenvalues of the system. In this case, they are the
diagonal elements of the matrix, 1 and -1. This mean that the origin is
a saddle point, and therefore it is unstable. In the \)</span>X<span class="math notranslate nohighlight">\( direction, the
emotions are going to grow without bound, either in the love or hate
direction, while in the \)</span>Y$ direction, the emotions are going to decay
to zero (indifference). This should be no surprise, that since the two
equations are independent, the lovers have no emotional effect on each
other.</p>
<p><strong>Lovers with no self-awareness:</strong> Here is an alternate situation:
suppose two lovers were not influenced by their own emotions, but were
instead attuned to the emotional state of the other. Then we might have
the following model, in which lover <span class="math notranslate nohighlight">\(X\)</span> reacts in the opposite way by
emotions of lover <span class="math notranslate nohighlight">\(Y\)</span>, but lover <span class="math notranslate nohighlight">\(Y\)</span> is, contrariwise, spurred by the
love or hate of <span class="math notranslate nohighlight">\(X\)</span> in the same direction: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot  X &amp; = &amp; -Y \\
\dot  Y &amp; = &amp;  X\end{aligned}\)</span><span class="math notranslate nohighlight">\( We find the eigenvalues of the system by
using the expression in equation \[eg:2D\_eig\]:
\)</span>\lambda =  (0 \pm \sqrt{-4})/2 = \pm i$. Pure imaginary eigenvalues
tell us that the origin is a center point, with the solutions periodic
orbits around the origin. Psychologically, we can interpret this
scenario as cycles of love and hate, never growing and never decaying.
The magnitude of these oscillations depends on the initial state of the
system, that is, the feelings the lovers had at the beginning of the
relationship.</p>
<p>We can now address the second question, and find under what
circumstances different types of dynamic behaviors occur. We consider
the general model, and ask what kinds of eigenvalues are possible for
different parameter values. First, we write down the general expression
for the eigenvalues, from equation [eg:2D_eig]:
$<span class="math notranslate nohighlight">\(\lambda =  \frac{a+d \pm \sqrt{(a+d)^2-4(ad-bc)}}{2}\)</span><span class="math notranslate nohighlight">\( There are two
properties we are interested in: stability and existence of
oscillations. Recall that stability is determined by the sign of the
real part of the eigenvalues. If the square root is imaginary, then the
real part is simply the trace (\)</span>a+d$), but if the square root is real,
we have to consider the whole expression to determine stability. So let
us first state the condition for existence of oscillations (imaginary
square root):</p>
<ol>
<li><p>Complex eigenvalues: oscillatory solutions $<span class="math notranslate nohighlight">\(4(ad-bc) &gt; (a+d)^2\)</span><span class="math notranslate nohighlight">\( If
this expression holds, the square root is imaginary, and the
stability is determined by the sign of the trace. That is, if
\)</span>a+d &gt; 0<span class="math notranslate nohighlight">\(, the system is unstable, and will grow into unbounded love
or hate, but if \)</span>a+d &lt; 0<span class="math notranslate nohighlight">\(, then the system is stable, and will
spiral to indifference. The special case \)</span>a+d = 0$, such as we saw
above, means that strictly periodic love/hate cycles are the
solutions.</p></li>
<li><p>Real eigenvalues: exponential growth and/or decay
$<span class="math notranslate nohighlight">\(4(ad-bc) &lt; (a+d)^2\)</span><span class="math notranslate nohighlight">\( In this case, the square root is real, and no
oscillatory solutions exist. In order to determine whether this
implies exponential growth, decay, or a combination, we must weigh
the relative sizes of \)</span>(a+d)<span class="math notranslate nohighlight">\( and \)</span>\sqrt{(a+d)^2-4(ad-bc)}<span class="math notranslate nohighlight">\(. If
\)</span>|a+d| &gt; \sqrt{(a+d)^2-4(ad-bc)}<span class="math notranslate nohighlight">\(, then adding or subtracting the
square root does not change the sign of \)</span>(a+d)$: if it is negative,
both eigenvalues are negative, and the origin is a stable node, and
if the trace is positive, the origin is an unstable node.</p>
<p>However, if the absolute value of the root outweighs the absolute
value of the trace <span class="math notranslate nohighlight">\(|a+d| &lt; \sqrt{(a+d)^2-4(ad-bc)}\)</span> , then either
adding or subtracting the root will change the sign of the
eigenvalues. Therefore, one eigenvalue is positive and the other is
negative, and the origin is a saddle point. The emotions will run
unchecked in some preferred direction, possibly combining love and
hate of the two lovers.</p>
</li>
</ol>
<p>These conditions are not intuitive, and it took some work to express
them. The benefit is that now, given any values of the self-involvement
parameters <span class="math notranslate nohighlight">\(a,d\)</span> and the sensitivity parameters <span class="math notranslate nohighlight">\(b,c\)</span> we can predict the
long-term dynamics of the model relationship. Whether the results have
any bearing on reality, of course, depends on how well the reality is
described by these primitive assumptions.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dmitry Kondrashov<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>