

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.1. Flow in the phase plane &#8212; Mathematical Methods for Biology</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6.6. Phase portraits in Python" href="linear_pp_python.html" />
    <link rel="prev" title="6. Linear ODEs with two variables" href="linear_pp_intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Mathematical Methods for Biology</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_1var_intro.html">
   1. Models with one variable in discrete time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_nonlinear_intro.html">
   2. Nonlinear discrete-time dynamic models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ode_1var_intro.html">
   3. Models with one variable in continuous time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="graph_ode_intro.html">
   4. Graphical analysis of ordinary differential equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_higher_intro.html">
   5. Discrete models of higher order
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="linear_pp_intro.html">
   6. Linear ODEs with two variables
  </a>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.1. Flow in the phase plane
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="#eigenvectors-and-eigenvalues">
     6.2. Eigenvectors and eigenvalues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="#solutions-of-linear-two-variable-odes">
     6.3. Solutions of linear two-variable ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="#classification-of-linear-systems">
     6.4. Classification of linear systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="#dynamics-of-romantic-relationships">
     6.5. Dynamics of romantic relationships
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_pp_python.html">
     6.6. Phase portraits in Python
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forces_pot_intro.html">
   7. Forces and potentials in biological modeling
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/linear_pp_math.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activators-and-inhibitors-in-biochemical-reactions">
   6.1.1. activators and inhibitors in biochemical reactions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#phase-plane-portraits">
   6.1.2. phase plane portraits
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="flow-in-the-phase-plane">
<h1><span class="section-number">6.1. </span>Flow in the phase plane<a class="headerlink" href="#flow-in-the-phase-plane" title="Permalink to this headline">¶</a></h1>
<div class="section" id="activators-and-inhibitors-in-biochemical-reactions">
<h2><span class="section-number">6.1.1. </span>activators and inhibitors in biochemical reactions<a class="headerlink" href="#activators-and-inhibitors-in-biochemical-reactions" title="Permalink to this headline">¶</a></h2>
<p>Suppose two gene products (proteins) regulate each others’ expression. Activator protein <span class="math notranslate nohighlight">\(A\)</span> binds to the promoter of the gene for <span class="math notranslate nohighlight">\(I\)</span> and activates its expression, while inhibitor protein <span class="math notranslate nohighlight">\(I\)</span> binds to the promoter of the gene for <span class="math notranslate nohighlight">\(A\)</span> and inhibits its expression (here the variables stand for concentrations of the two proteins in the cell):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot  A &amp; = &amp; - \alpha I\\
\dot  I &amp; = &amp; \beta A\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are positive rate constants. They represent the rate of inhibition of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(I\)</span>, and of activation of <span class="math notranslate nohighlight">\(I\)</span> by <span class="math notranslate nohighlight">\(A\)</span>, respectively. Let us now complicate the model by adding self-inhibition. It is common for regulatory proteins to
inhibit their own production. Then, we have the following system of equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot  A &amp; = &amp; - \gamma A - \alpha I\\
\dot  I &amp; = &amp; \beta A - \delta I
\end{aligned}
\end{split}\]</div>
<p>Here we have added two rates of self-inhibition <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span>. This is a system of two coupled ODEs, and we will learn how to analyze these models both analytically and graphically.</p>
</div>
<div class="section" id="phase-plane-portraits">
<h2><span class="section-number">6.1.2. </span>phase plane portraits<a class="headerlink" href="#phase-plane-portraits" title="Permalink to this headline">¶</a></h2>
<p>Before we learn about the analytical tools of linear algebra, let us think intuitively about the effect of the variables on each other. The best way to describe this is through plotting the geometry of the <em>flow</em> prescribed by the differential equations. As we saw, for one-dimensional ODEs the direction of the change of the dependent variable (also known as the flow) could be shown as arrows on a line. A single variable can only increase, decrease, or stay the same (at a fixed point). In two dimensions there is more freedom. The flow is plotted on the <em>phase plane</em>, where for any combination of the two variables (say <span class="math notranslate nohighlight">\(x,y\)</span>) the ODE gives the derivatives of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This vector gives the flow, or the rate of change at the particular point in the plane. Intuitively, the flow describes the direction in which the system is pulling the 2-dimensional solution. If we plot the progress of a solution of ODE (all the values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> starting with the initial condition) we will obtain a <em>trajectory</em> in the phase plane. The arrows of the flow are tangent to any trajectory curve, since they plot the derivatives of <span class="math notranslate nohighlight">\(x,y\)</span>.</p>
<p><strong>Example: positive relationship between the variables</strong> Consider the following system of differential equations:</p>
<div class="math notranslate nohighlight" id="equation-eq-ode1">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-eq-ode1" title="Permalink to this equation">¶</a></span>\[\begin{split}
\\begin{aligned}
\dot x &amp; = &amp; x + y \\
\dot y &amp; = &amp; x + 2y 
\end{aligned}
\end{split}\]</div>
<p>This is system is coupled, with <span class="math notranslate nohighlight">\(x\)</span> having an effect on <span class="math notranslate nohighlight">\(y\)</span> and vice versa. Specifically, the signs of the constants mean that positive values of <span class="math notranslate nohighlight">\(x\)</span> have the effect of increasing <span class="math notranslate nohighlight">\(y\)</span> (and vice versa), while negative values of <span class="math notranslate nohighlight">\(x\)</span> have the effect of decreasing <span class="math notranslate nohighlight">\(y\)</span> (and vice versa). For any pair of values of <span class="math notranslate nohighlight">\((x,y)\)</span>, there is a flow prescribed by the ODEs. E.g., when <span class="math notranslate nohighlight">\(x=1, y=1\)</span>, the derivatives are <span class="math notranslate nohighlight">\(\dot x = 2, \dot y = 3\)</span>. This means that the flow at that point is given by the vector <span class="math notranslate nohighlight">\((2,3)\)</span>, and can be plotted in the <span class="math notranslate nohighlight">\(x,y\)</span> phase plane. This can be done for any pair of values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and plotted to give the phase plane portrait in figure <a class="reference internal" href="#fig-ode1"><span class="std std-numref">Fig. 6.1</span></a>.</p>
<div class="figure align-default" id="fig-ode1">
<img alt="_images/week6_pp5.png" src="_images/week6_pp5.png" />
<p class="caption"><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Phase plane flow for the system in <a class="reference internal" href="#equation-eq-ode1">(6.1)</a></span><a class="headerlink" href="#fig-ode1" title="Permalink to this image">¶</a></p>
</div>
<p>Observe that the overall dynamics of the systems are directed outward from the origin, as we expect from the ODEs. The blue lines on the plot are some sample trajectories. The solution over time for both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> will either grow toward positive infinity, or decay to negative infinity.</p>
<p><strong>Example: negative relationship between the variables</strong> Consider the following system of differential equations, where <span class="math notranslate nohighlight">\(y\)</span> has an effect on <span class="math notranslate nohighlight">\(\dot x\)</span> opposite of its own sign. That is, negative values of <span class="math notranslate nohighlight">\(y\)</span> contribute to the growth of <span class="math notranslate nohighlight">\(x\)</span>, and vice versa.</p>
<div class="math notranslate nohighlight" id="equation-eq-ode2">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-eq-ode2" title="Permalink to this equation">¶</a></span>\[\begin{split}
\begin{aligned}
\dot x &amp; = &amp; -y \\
\dot y &amp; = &amp; x  
\end{aligned}
\end{split}\]</div>
<p>As above, the flow at any one point is given by the ODEs. E.g. at <span class="math notranslate nohighlight">\((0,1)\)</span> the two derivatives prescribe flow in the <span class="math notranslate nohighlight">\((1,0)\)</span> (up) direction, while at <span class="math notranslate nohighlight">\((1,0)\)</span> the flow is in the <span class="math notranslate nohighlight">\((0,-1)\)</span> direction. Figure <a class="reference internal" href="#fig-ode2"><span class="std std-numref">Fig. 6.2</span></a> shows the arrows of flow in the phase plane around the origin. Note that the arrows go around in a circular pattern around the origin - this shows oscillatory flow of solutions.</p>
<div class="figure align-default" id="fig-ode2">
<img alt="_images/week6_pp2.png" src="_images/week6_pp2.png" />
<p class="caption"><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Phase plane flow for the system in <a class="reference internal" href="#equation-eq-ode2">(6.2)</a></span><a class="headerlink" href="#fig-ode2" title="Permalink to this image">¶</a></p>
</div>
<p>Let us consider the trajectories of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> in time. The blue curves in the phase plane plot demonstrate the solutions go around the origin and return to the same point. This means that the behavior of the solutions over time is <em>periodic</em>, with oscillations going from positive to negative numbers and back forever.</p>
</div>
</div>
<div class="section" id="eigenvectors-and-eigenvalues">
<h1><span class="section-number">6.2. </span>Eigenvectors and eigenvalues<a class="headerlink" href="#eigenvectors-and-eigenvalues" title="Permalink to this headline">¶</a></h1>
<div class="section" id="basic-linear-algebra-terminology">
<h2><span class="section-number">6.2.1. </span>basic linear algebra terminology<a class="headerlink" href="#basic-linear-algebra-terminology" title="Permalink to this headline">¶</a></h2>
<p>We have seen matrix notation introduced in the previous chapter, along with the definition of matrix multiplication. One basic advantage of this notation is that it makes it possible to write any set of <em>linear equations</em> as a single matrix equation. By linear equations we mean those that contain only constants or first powers of the variables. The field of mathematics studying matrices and their generalizations is called <em>linear algebra</em>; it is fundamental to both pure and applied mathematics. In this section we will learn some basic facts about matrices and their properties. First of all, let us define some basic terms:</p>
<div class="admonition-matrix-definition admonition">
<p class="admonition-title">Matrix definition</p>
<ul class="simple">
<li><p>A matrix <span class="math notranslate nohighlight">\(A\)</span> is a rectangular array of <em>elements</em> <span class="math notranslate nohighlight">\(A_{ij}\)</span>, in which <span class="math notranslate nohighlight">\(i\)</span> denotes the row number (index), counted from the top, and <span class="math notranslate nohighlight">\(j\)</span> denotes the column number (index), counted from left to right.</p></li>
<li><p>The elements of a matrix <span class="math notranslate nohighlight">\(A\)</span> which have the same row and column index, e.g. <span class="math notranslate nohighlight">\(A_{33}\)</span> are called the <em>diagonal elements</em>. Those which do not lie on the diagonal are called the <em>off-diagonal</em> elements.</p></li>
<li><p>The <em>trace</em> <span class="math notranslate nohighlight">\(\tau\)</span> of a matrix <span class="math notranslate nohighlight">\(A\)</span> is the sum of the diagonal elements: <span class="math notranslate nohighlight">\(\tau = \sum_i A_{ii}\)</span></p></li>
<li><p>The <em>determinant</em> <span class="math notranslate nohighlight">\(\Delta\)</span> of a 2x2 matrix <span class="math notranslate nohighlight">\(A\)</span> is given by the following: <span class="math notranslate nohighlight">\(\Delta = ad - bc\)</span>, where</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)
\end{split}\]</div>
<p>For larger matrices, the determinant is defined recursively, in terms of 2x2 submatrices of the larger matrix, but we will not give the full definition here.</p>
</div>
<p>For example, in the 3 x 3 matrix below, the elements <span class="math notranslate nohighlight">\(a, e, i\)</span> are the diagonal elements:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left(\begin{array}{ccc}a &amp; b &amp; c \\d &amp; e &amp; f \\g &amp; h &amp; i\end{array}\right)
\end{split}\]</div>
<p>In chapter 3 you learned the rule of matrix multiplication, and we can write <span class="math notranslate nohighlight">\(C = A \times B\)</span>, so long as the number of columns in <span class="math notranslate nohighlight">\(A\)</span> matches the number of rows in <span class="math notranslate nohighlight">\(B\)</span>. However, what if we want to reverse the process? If we know the resulting matrix <span class="math notranslate nohighlight">\(C\)</span>, and one of the two matrices, e.g. <span class="math notranslate nohighlight">\(A\)</span>, how can we find <span class="math notranslate nohighlight">\(B\)</span>? Naively, we would like to be able to divide both sides by the matrix <span class="math notranslate nohighlight">\(A\)</span>, and find <span class="math notranslate nohighlight">\(B = C/A\)</span>. However, things are more complicated for matrices.</p>
<p>Properly speaking, we need to introduce the <em>inverse</em> of a matrix <span class="math notranslate nohighlight">\(A\)</span>. If we think about inverses of real numbers, <span class="math notranslate nohighlight">\(a^{-1}\)</span> is a number that when it multiplies <span class="math notranslate nohighlight">\(a\)</span>, results in one. In order to define the equivalent for matrices, we first need to introduce the unity of matrix multiplication.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The <em>identity</em> matrix is an <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix that does not change another <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix by multiplication:</p>
<div class="math notranslate nohighlight">
\[
A \times I = I \times A  = A
\]</div>
<p>The diagonal elements of the identity matrix are 1s and all off-diagonal elements are zero.</p>
</div>
<p><strong>Example:</strong> Useing the definition of matrix multiplication from chapter 3, we can verify that this definition works for any 2 by 2 matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}-6 &amp; -2 \\ 12 &amp; -1 \end{pmatrix} \times \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix} = 
\begin{pmatrix}-6\times 1 + -2\times 0 &amp; -6\times 0 + -2\times 1  \\ 12\times 1 -1 \times 0 &amp; 12\times 0 -1 \times 1  \end{pmatrix} = \begin{pmatrix}-6 &amp; -2 \\ 12 &amp; -1 \end{pmatrix}
\end{split}\]</div>
<p>Now that we have specified the identity, we can define the matrix inverse:</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>A square matrix <span class="math notranslate nohighlight">\(A\)</span> has an <em>inverse matrix</em> <span class="math notranslate nohighlight">\(A^{-1}\)</span> if it satisfies the following:</p>
<div class="math notranslate nohighlight">
\[
A^{-1} A = A A^{-1} = I
\]</div>
</div>
<p>Finding the inverse of a matrix is not simple, and we will be content to let computers handle the dirty work. In fact, not every matrix possesses an inverse. There is a test for existence of an inverse of <span class="math notranslate nohighlight">\(A\)</span>, and it depends on the determinant [&#64;strang_linear_2005]:</p>
<p>A square matrix <span class="math notranslate nohighlight">\(A\)</span> possesses an inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span> and is called <em>invertible</em> if and only if its determinant is not zero.</p>
</div>
<div class="section" id="matrices-transform-vectors">
<h2><span class="section-number">6.2.2. </span>matrices transform vectors<a class="headerlink" href="#matrices-transform-vectors" title="Permalink to this headline">¶</a></h2>
<p>In this section we will learn to characterize square matrices by finding special numbers and vectors associated with them. At the core of this analysis lies the concept of a matrix as an <em>operator</em> that transforms vectors by multiplication, as we defined in chapter 3. To be clear, in this section we take as default that the matrices <span class="math notranslate nohighlight">\(A\)</span> are square, and that vectors <span class="math notranslate nohighlight">\(\vec v\)</span> are column vectors, and thus will multiply the matrix on the right: <span class="math notranslate nohighlight">\(A \times  \vec v\)</span>.</p>
<p>A matrix multiplied by a vector produces another vector, provided the number of columns in the matrix is the same as the number of rows in the vector. This can be interpreted as the matrix transforming the vector <span class="math notranslate nohighlight">\(\vec v\)</span> into another one: <span class="math notranslate nohighlight">\( A  \times  \vec v = \vec u\)</span>. The resultant vector <span class="math notranslate nohighlight">\(\vec u\)</span> may or may not resemble <span class="math notranslate nohighlight">\(\vec v\)</span>, but there are special vectors for which the transformation is very simple.</p>
<p><strong>Example.</strong> Let us multiply the following matrix and vector (specially chosen to make a point):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ -1 \end{array}\right) = \left(\begin{array}{c}2 -1 \\ 2 - 3 \end{array}\right) =  \left(\begin{array}{c} 1 \\ -1 \end{array}\right)
\end{split}\]</div>
<p>We see that this particular vector is unchanged when multiplied by this matrix, or we can say that the matrix multiplication is equivalent to multiplication by 1. Here is another such vector for the same matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)\left(\begin{array}{c}1 \\ 2 \end{array}\right) = \left(\begin{array}{c}2 +2 \\ 2 + 6 \end{array}\right) =  \left(\begin{array}{c} 4 \\ 8 \end{array}\right)
\end{split}\]</div>
<p>In this case, the vector is changed, but only by multiplication by a constant (4). Thus the geometric direction of the vector remained unchanged.</p>
<p>Generally, a square matrix has an associated set of vectors for which multiplication by the matrix is equivalent to multiplication by a constant. This can be written down as a definition:</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>An <em>eigenvector</em> of a square matrix <span class="math notranslate nohighlight">\(A\)</span> is a vector <span class="math notranslate nohighlight">\(\vec v\)</span> for which matrix multiplication by <span class="math notranslate nohighlight">\(A\)</span> is equivalent to multiplication by a constant. This constant <span class="math notranslate nohighlight">\(\lambda\)</span> is called its <em>eigenvalue</em> of <span class="math notranslate nohighlight">\(A\)</span> corresponding the the eigenvector <span class="math notranslate nohighlight">\(\vec v\)</span>. The relationship is summarized in the following equation:</p>
<div class="math notranslate nohighlight" id="equation-def-eigen">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-def-eigen" title="Permalink to this equation">¶</a></span>\[
A  \times  \vec v = \lambda \vec v
\]</div>
</div>
<p>Note that this equation combines a matrix (<span class="math notranslate nohighlight">\(A\)</span>), a vector (<span class="math notranslate nohighlight">\(\vec v\)</span>) and a scalar <span class="math notranslate nohighlight">\(\lambda\)</span>, and that both sides of the equation are column vectors. This definition is illustrated in <a class="reference internal" href="#fig-eig-vec"><span class="std std-numref">Fig. 6.3</span></a>, showing a vector (<span class="math notranslate nohighlight">\(v\)</span>) multiplied by a matrix <span class="math notranslate nohighlight">\(A\)</span>, and the resulting vector <span class="math notranslate nohighlight">\(\lambda v\)</span>, which is in the same direction as <span class="math notranslate nohighlight">\(v\)</span>, due to scalar multiplying all elements of a vector, thus either stretching it if <span class="math notranslate nohighlight">\(\lambda&gt;1\)</span> or compressing it if <span class="math notranslate nohighlight">\(\lambda &lt; 1\)</span>. This assumes that <span class="math notranslate nohighlight">\(\lambda\)</span> is a real number, which is not always the case, but we will leave that complication aside for the purposes of this chapter.</p>
<div class="figure align-default" id="fig-eig-vec">
<img alt="_images/Eigenvalue_equation.png" src="_images/Eigenvalue_equation.png" />
<p class="caption"><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Illustration of the geometry of a matrix <span class="math notranslate nohighlight">\(A\)</span> multiplying its eigenvector <span class="math notranslate nohighlight">\(v\)</span>, resulting in a vector in the same direction
<span class="math notranslate nohighlight">\(\lambda v\)</span>. (Figure by Lantonov under CC BY-SA 4.0 via Wikimedia Commons)</span><a class="headerlink" href="#fig-eig-vec" title="Permalink to this image">¶</a></p>
</div>
<p>The definition does not specify how many such eigenvectors and eigenvalues can exist for a given matrix <span class="math notranslate nohighlight">\(A\)</span>. There are usually as many such vectors <span class="math notranslate nohighlight">\(\vec v\)</span> and corresponding numbers <span class="math notranslate nohighlight">\(\lambda\)</span> as the number of rows or columns of the square matrix <span class="math notranslate nohighlight">\(A\)</span>, so a 2 by 2 matrix has two eigenvectors and two eigenvalues, a 5x5 matrix has 5 of each, etc. One ironclad rule is that there cannot be more distinct eigenvalues than the matrix dimension. Some matrices possess fewer eigenvalues than the matrix dimension, those are said to have a <em>degenerate</em> set of eigenvalues, and at least two of the eigenvectors share the same eigenvalue.</p>
<p>The situation with eigenvectors is trickier. There are some matrices for which any vector is an eigenvector, and others which have a limited set of eigenvectors. What is difficult about counting eigenvectors is that an eigenvector is still an eigenvector when multiplied by a constant. You can show that for any matrix, multiplication by a constant is commutative: <span class="math notranslate nohighlight">\(cA = Ac\)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is a matrix and <span class="math notranslate nohighlight">\(c\)</span> is a constant. This leads us to the important result that if <span class="math notranslate nohighlight">\(\vec v\)</span> is an eigenvector with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, then any scalar multiple <span class="math notranslate nohighlight">\(c \vec v\)</span> is also an eigenvector with the same eigenvalue. The following demonstrates this algebraically:</p>
<div class="math notranslate nohighlight">
\[
A  \times  (c \vec v) = c A  \times  \vec v = c \lambda \vec v =  \lambda (c \vec v)
\]</div>
<p>This shows that when the vector <span class="math notranslate nohighlight">\(c \vec v\)</span> is multiplied by the matrix <span class="math notranslate nohighlight">\(A\)</span>, it results in its being multiplied by the same number <span class="math notranslate nohighlight">\(\lambda\)</span>, so by definition it is an eigenvector. Therefore, an eigenvector <span class="math notranslate nohighlight">\(\vec v\)</span> is not unique, as any constant multiple <span class="math notranslate nohighlight">\(c \vec v\)</span> is also an eigenvector. It is more useful to think not of a single eigenvector <span class="math notranslate nohighlight">\(\vec v\)</span>, but of a <strong>collection of vectors that can be interconverted by scalar multiplication</strong> that are all essentially the same eigenvector. Another way to represent this, if the eigenvector is real, is that an eigenvector as a <strong>direction that remains unchanged by multiplication by the matrix</strong>, such as direction of the vector <span class="math notranslate nohighlight">\(v\)</span> in figure . As mentioned above, this is true only for real eigenvalues and eigenvectors, since complex eigenvectors cannot be used to define a direction in a real space.</p>
<p>To summarize, eigenvalues and eigenvectors of a matrix are a set of numbers and a set of vectors (up to scalar multiple) that describe the action of the matrix as a multiplicative operator on vectors. “Well-behaved” square <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrices have <span class="math notranslate nohighlight">\(n\)</span> distinct eigenvalues and <span class="math notranslate nohighlight">\(n\)</span> eigenvectors pointing in distinct directions. In a deep sense, the collection of eigenvectors and eigenvalues defines a matrix <span class="math notranslate nohighlight">\(A\)</span>, which is why an older name for them is characteristic vectors and values.</p>
</div>
<div class="section" id="calculating-eigenvalues">
<h2><span class="section-number">6.2.3. </span>calculating eigenvalues<a class="headerlink" href="#calculating-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>Finding the eigenvalues and eigenvectors analytically, that is on paper, is quite laborious even for 3 by 3 or 4 by 4 matrices and for larger ones there is no analytical solution. In practice, the task is outsourced to a computer, and MATLAB has a number of functions for this purpose. Nevertheless, it is useful to go through the process in 2 dimensions in order to gain an understanding of what is involved. From the definition [def:eigen] of eigenvalues and eigenvectors, the condition can be written in terms of the four elements of a 2 by 2 matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)
\end{split}\]</div>
<p>This is now a system of two linear algebraic equations, which we can solve by substitution. First, let us solve for <span class="math notranslate nohighlight">\(v_1\)</span> in the first row, to get</p>
<div class="math notranslate nohighlight">
\[
v_1 = \frac{-bv_2}{a-\lambda}
\]</div>
<p>Then we substitute this into the second equation and get:</p>
<div class="math notranslate nohighlight">
\[
\frac{-bcv_2}{a-\lambda} +(d-\lambda)v_2 = 0
\]</div>
<p>Since <span class="math notranslate nohighlight">\(v_2\)</span> multiplies both terms, and is not necessarily zero, we require that its multiplicative factor be zero. Doing a little algebra, we obtain the following, known as the <em>characteristic equation</em> of the matrix:</p>
<div class="math notranslate nohighlight">
\[
-bc +(a-\lambda)(d-\lambda) = \lambda^2-(a+d)\lambda +ad-bc = 0
\]</div>
<p>This equation can be simplified by using two quantities we defined at the beginning of the section: the sum of the diagonal elements called the trace <span class="math notranslate nohighlight">\(\tau = a+d\)</span>, and the determinant <span class="math notranslate nohighlight">\(\Delta = ad-bc\)</span>. The quadratic equation has two solutions, dependent solely on <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\Delta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\lambda = \frac{\tau \pm \sqrt{\tau^2-4\Delta}}{2}
\]</div>
<p>This is the general expression for a 2 by 2 matrix, showing there are two possible eigenvalues. Note that if <span class="math notranslate nohighlight">\(\tau^2-4\Delta&gt;0\)</span>, the eigenvalues are real, if <span class="math notranslate nohighlight">\(\tau^2-4\Delta&lt;0\)</span>, they are complex (have real and imaginary parts), and if <span class="math notranslate nohighlight">\(\tau^2-4\Delta=0\)</span>, there is only one eigenvalue. This situation is known as degenerate, because two eigenvectors share the same eigenvalue.</p>
<p><strong>Example.</strong> Let us take the same matrix we looked at in the previous subsection:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)
\end{split}\]</div>
<p>The trace of this matrix is <span class="math notranslate nohighlight">\(\tau = 2+3 =5\)</span> and the determinant is <span class="math notranslate nohighlight">\(\Delta = 6 - 2 = 4\)</span>. Then by our formula, the eigenvalues are:</p>
<div class="math notranslate nohighlight">
\[
\lambda = \frac{5 \pm \sqrt{5^2-4 \times 4}}{2}  =  \frac{5 \pm 3}{2}  = 4, 1
\]</div>
<p>These are the multiples we found in the example above, as expected.</p>
<p>A real matrix can have complex eigenvalues and eigenvectors, but whenever it acts on a real vector, the result is still real. This is because the complex numbers cancel each other’s imaginary parts. For discrete time models, it is enough to consider the absolute value of a complex eigenvalue, which is defined as following: <span class="math notranslate nohighlight">\(|a +b i|= \sqrt{a^2 + b^2}\)</span>. As before, the eigenvalue with the largest absolute value “wins” in the long term.</p>
</div>
<div class="section" id="calculation-of-eigenvectors-on-paper">
<h2><span class="section-number">6.2.4. </span>calculation of eigenvectors on paper<a class="headerlink" href="#calculation-of-eigenvectors-on-paper" title="Permalink to this headline">¶</a></h2>
<p>The surprising fact is that, as we saw in the last subsection, the eigenvalues of a matrix can be found without knowing its eigenvectors! However, the converse is not true: to find the eigenvectors, one first needs to know the eigenvalues. Given an eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, let us again write down the defining equation of the eigenvector for a generic 2 by 2 matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}v_1 \\ v_2 \end{array}\right) = \left(\begin{array}{c}av_1 +b v_2\\ cv_1+ dv_2 \end{array}\right) = \lambda \left(\begin{array}{c}v_1 \\ v_2 \end{array}\right)
\end{split}\]</div>
<p>This vector equation is equivalent to two algebraic equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
av_1 + b v_2 &amp;= \lambda v_1 \\
cv_1 + d v_2 &amp;= \lambda v_2 
\end{aligned}
\end{split}\]</div>
<p>Since we’ve already found <span class="math notranslate nohighlight">\(\lambda\)</span> by solving the characteristic equation, this is two linear equations with two unknowns (<span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span>). You may remember from advanced algebra that such equations may either have a single solution for each unknown, but sometimes they may have none, or infinitely many solutions. Since there are unknowns on both sides of the equation, we can make both equations be equal to zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(a-\lambda)v_1 + b v_2 &amp;= 0 \\
cv_1 + (d-\lambda ) v_2 &amp;=0
\end{aligned}
\end{split}\]</div>
<p>So the first equation yields the relationship <span class="math notranslate nohighlight">\(v_1 = -v_2 b/(a-\lambda)\)</span> and the second equation is <span class="math notranslate nohighlight">\(v_1 = -v_2(d-\lambda)/c\)</span>, which we already obtained in the last subsection. We know that these two equations must be the same, since the ratio of <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> is what defines the eigenvector. So we can use either expression to find the eigenvector.</p>
<p><strong>Example.</strong> Let us return to the same matrix we looked at in the previous subsection:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left(\begin{array}{cc}2 &amp; 1 \\ 2&amp; 3\end{array}\right)
\end{split}\]</div>
<p>The eigenvalues of the matrix are 1 and 4. Using our expression above, where the element <span class="math notranslate nohighlight">\(a=2\)</span> and <span class="math notranslate nohighlight">\(b=1\)</span>, let us find the eigenvector corresponding to the eigenvalue 1:</p>
<div class="math notranslate nohighlight">
\[
v_1 = - v_2 \times  1/(2-1) = - v_2
\]</div>
<p>Therefore the eigenvector is characterized by the first and second elements being negatives of each other. We already saw in the example two subsections above that the vector <span class="math notranslate nohighlight">\((1,-1)\)</span> is such as eigenvector, but it is also true of the vectors <span class="math notranslate nohighlight">\((-1,1)\)</span>, <span class="math notranslate nohighlight">\((-\pi, \pi)\)</span> and <span class="math notranslate nohighlight">\((10^6, -10^6)\)</span>. This infinite collection of vectors, all along the same direction, can be described as the eigenvector (or eigendirection) corresponding to the eigenvalue 1.</p>
<p>Repeating this procedure for <span class="math notranslate nohighlight">\(\lambda = 4\)</span>, we obtain the linear relationship: $<span class="math notranslate nohighlight">\(v_1 = - v_2 \times  1/(2-4) = 0.5 v_2\)</span><span class="math notranslate nohighlight">\( Once again, the example vector we saw two subsections \)</span>(2,1)<span class="math notranslate nohighlight">\( is in agreement with our calculation. Other vectors that satisfy this relationship include \)</span>(10,5)<span class="math notranslate nohighlight">\(, \)</span>(-20,-10)<span class="math notranslate nohighlight">\(, and \)</span>(-0.4,-0.2)$. This is again a collection of vectors that are all considered the same eigenvector with eigenvalue 4 which are all pointing in the same direction, with the only difference being their length.</p>
</div>
</div>
<div class="section" id="solutions-of-linear-two-variable-odes">
<h1><span class="section-number">6.3. </span>Solutions of linear two-variable ODEs<a class="headerlink" href="#solutions-of-linear-two-variable-odes" title="Permalink to this headline">¶</a></h1>
<p>Let us start by considering two variable ODEs that do not affect each other:</p>
<p><strong>Example: two uncoupled ODEs</strong>  In general, for a two-variable system, the value of one variable affects the other. In the equations above, the terms with the constants <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> provide what is known as <em>coupling</em> between the two variables. Let us look at the primitive situation where the two variables are uncoupled, as an illustration of solving two-dimensional ODEs. If we set the coupling constants <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> to 0, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot x &amp; = &amp; ax \\
\dot y &amp; = &amp; dy
\end{aligned}
\end{split}\]</div>
<p>Using our knowledge of 1D linear ODE, we can solve the two equations independently to get the following: <span class="math notranslate nohighlight">\(x(t) = x_0 e^{at}\)</span> and <span class="math notranslate nohighlight">\(y(t) = y_0 e^{dt}\)</span>. The solutions can be written in vector form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{c}x(t) \\y(t)\end{array}\right) =
x_0 e^{at} \left(\begin{array}{c}1 \\0\end{array}\right)+y_0 e^{dt}\left(\begin{array}{c}0\\1\end{array}\right)
\end{split}\]</div>
<p>This is another way of writing that the dynamics of variable <span class="math notranslate nohighlight">\(x\)</span> is exponential growth (or decay) with rate <span class="math notranslate nohighlight">\(a\)</span>, and ditto <span class="math notranslate nohighlight">\(y\)</span>, with rate <span class="math notranslate nohighlight">\(d\)</span>. Given the initial conditions <span class="math notranslate nohighlight">\((x_0, y_0)\)</span>, we can divide the behavior of the solutions into a sum of two vectors, each growing or decaying at its own rate.</p>
<p>Linear algebra allows us to find the solution for two-dimensional ODEs where the variables are interdependent using the same idea. The general (homogeneous) ODE with two dependent variables can be written as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot x &amp; = &amp; ax + by \\
\dot y &amp; = &amp; cx + dy
\end{aligned}
\end{split}\]</div>
<p>We can write this in matrix form like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{c}\dot x \\ \dot y \end{array}\right) = \left(\begin{array}{cc}a &amp; b \\c &amp; d\end{array}\right)\left(\begin{array}{c}x \\ y \end{array}\right)
\end{split}\]</div>
<p>Let us call the matrix <span class="math notranslate nohighlight">\(A\)</span>, and represent the vector <span class="math notranslate nohighlight">\((x,y)\)</span> as <span class="math notranslate nohighlight">\(\vec {x}\)</span>, then the general linear equation can be written like this:</p>
<div class="math notranslate nohighlight" id="equation-gen-lin-mult">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-gen-lin-mult" title="Permalink to this equation">¶</a></span>\[
\dot{ \vec{ x}} = A \vec{x}
\]</div>
<p>This notation is intended to make plain the similarity with the linear 1D ODE: <span class="math notranslate nohighlight">\(\dot x = a x\)</span>. This similarity is deep and substantial, in that linear equations in multiple dimensions share the same basic exponential form. In general, all solutions of linear equations can be written as a sum of exponentials multiplying different vectors:</p>
<div class="tip admonition">
<p class="admonition-title">General solution of linear 2-variable ODEs</p>
<div class="math notranslate nohighlight" id="equation-gen-sol-2var">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-gen-sol-2var" title="Permalink to this equation">¶</a></span>\[\begin{split}
\left(\begin{array}{c} x(t) \\  y(t) \end{array}\right) =C_1e^{\lambda_1 t} \left(\begin{array}{c}x_1\\y_1\end{array}\right)+C_2 e^{\lambda_2 t}\left(\begin{array}{c}x_2\\y_2\end{array}\right)
\end{split}\]</div>
<p>The constants <span class="math notranslate nohighlight">\(C_1, C_2\)</span> are determined by the initial conditions, while the constants <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2\)</span> are the eigenvalues and the vectors <span class="math notranslate nohighlight">\((x_1,y_1)\)</span> and <span class="math notranslate nohighlight">\((x_2,y_2)\)</span> are the eigenvectors of the matrix <span class="math notranslate nohighlight">\(A\)</span>. We will now consider the application of this general result using computational tools.</p>
</div>
</div>
<div class="section" id="classification-of-linear-systems">
<h1><span class="section-number">6.4. </span>Classification of linear systems<a class="headerlink" href="#classification-of-linear-systems" title="Permalink to this headline">¶</a></h1>
<p>We have seen that linear algebra allows us to write down the solution of a multivariable dynamical system into a sum of exponential terms. In this section we use computational techniques to find the eigenvalues and eigenvectors of a system, and then produce the <em>phase portraits</em> of the linear systems. There are only a few different types of flow possible for linear systems, and we will classify them.</p>
<div class="section" id="real-eigenvalues">
<h2><span class="section-number">6.4.1. </span>real eigenvalues<a class="headerlink" href="#real-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>Let us consider the fixed points of the linear system: since both <span class="math notranslate nohighlight">\(\dot x =0 \)</span> and <span class="math notranslate nohighlight">\(\dot y = 0\)</span> must be zero, the only fixed point is the origin <span class="math notranslate nohighlight">\((0,0)\)</span>. We will see that the stability of the fixed point depends on the sign of the real part of the eigenvalue.</p>
<p>Suppose we have a positive real eigenvalue. The solution in the direction of the corresponding eigenvector is then described by <span class="math notranslate nohighlight">\(Ce^{\lambda t}\)</span>, <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>, which is exponential growth. The means that the solution is going to grow in the direction of the eigenvector away from the origin, and thus the origin is an unstable fixed point (in this direction). This type of fixed point is called an <em>unstable node</em>.</p>
<p>On the other hand, if <span class="math notranslate nohighlight">\(\lambda &lt; 0\)</span> for both eigenvalues, the solution decays exponentially and thus approaches the origin, so the fixed point is stable. This type of fixed point is called a <em>stable node</em>.</p>
<p>Since there are two different eigenvalues, one may be positive while another is negative. In this case, the fixed point is is called a <em>saddle point</em> for geometric reasons: solutions flow toward it in one direction, like a marble along the forward-backward axis of a saddle on a horse and flow away from it along the sideways direction on a saddle. Then, the fixed point is stable when approached along one eigenvector, but unstable along the other. What happens if the initial condition is not on either eigenvector? I will use a fact of linear algebra that given any two (non-colinear) 2D vectors, any vector in the plane can represented as a sum (with some coefficients) of these two. Thus, the general solution can be written as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\begin{array}{c} x(t) \\  y(t) \end{array}\right) =C_1e^{at} \left(\begin{array}{c}v_1\\v_2\end{array}\right)+C_2 e^{-bt}\left(\begin{array}{c}u_1\\u_2\end{array}\right)
\end{split}\]</div>
<div class="figure align-default" id="fig-saddle">
<img alt="_images/week6_pp1.png" src="_images/week6_pp1.png" />
<p class="caption"><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Phase plane flow for a linear system with a saddle point</span><a class="headerlink" href="#fig-saddle" title="Permalink to this image">¶</a></p>
</div>
<p>where <span class="math notranslate nohighlight">\(a,b&gt;0\)</span>. Then we see that the component in the direction of the first eigenvector will grow, while the component along the second eigenvector will decay. Thus, as <span class="math notranslate nohighlight">\(t \rightarrow \infty\)</span>, all solutions will approach the vector with the unstable eigenvalue, except those with initial conditions right on the eigenvector corresponding to the stable eigenvalue. This means that the fixed point is essentially unstable, because only trajectories which start exactly along the stable direction approach the fixed point in the long run, while others, may approach the fixed point for a finite time, flow away when the unstable component with the positive eigenvalue takes over, as shown in figure .</p>
</div>
<div class="section" id="complex-eigenvalues">
<h2><span class="section-number">6.4.2. </span>complex eigenvalues<a class="headerlink" href="#complex-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>If the argument of the square root is negative, eigenvalues may be complex numbers, which we can write like this: <span class="math notranslate nohighlight">\(a+bi\)</span>. Using Euler’s formula, we can write down the time-dependent part of the solutions as the following:</p>
<div class="math notranslate nohighlight">
\[
e^{(a + bi)t} = e^{at}e^{bit}= e^{at}(\cos(bt)+i\sin(bt))
\]</div>
<p>The behavior of these solutions combines exponential growth or decay from the real part, with the oscillations produced by the imaginary part. This describes either exponentially growing or decaying oscillations, which look like decaying waves in time, or as a spiral in the phase plane:</p>
<div class="figure align-default" id="fig-exp-decay">
<img alt="_images/lec7_exp_osc.png" src="_images/lec7_exp_osc.png" />
<p class="caption"><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Exponentially decaying oscillations in the time plot of the solution <span class="math notranslate nohighlight">\(x(t)\)</span></span><a class="headerlink" href="#fig-exp-decay" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-stab-spiral">
<img alt="_images/lec7_pp1.png" src="_images/lec7_pp1.png" />
<p class="caption"><span class="caption-number">Fig. 6.6 </span><span class="caption-text">Phase plane portrait of a stable spiral</span><a class="headerlink" href="#fig-stab-spiral" title="Permalink to this image">¶</a></p>
</div>
<p>Thus we see that the stability of the fixed point with complex eigenvalues depends on the sign of the real part. Purely imaginary eigenvalues produce periodic oscillations, which keep the same amplitude, as we saw in the example in the modeling section.</p>
</div>
<div class="section" id="id1">
<h2><span class="section-number">6.4.3. </span>classification of linear systems<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Stability</p></th>
<th class="head"><p>positive real parts</p></th>
<th class="head"><p>negative real parts</p></th>
<th class="head"><p>one positive, one negative</p></th>
<th class="head"><p>zero real part</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>real:</p></td>
<td><p>unstable node</p></td>
<td><p>stable node</p></td>
<td><p>saddle point</p></td>
<td><p>fixed line</p></td>
</tr>
<tr class="row-odd"><td><p>complex:</p></td>
<td><p>unstable spiral</p></td>
<td><p>stable spiral</p></td>
<td><p>N/A</p></td>
<td><p>center point</p></td>
</tr>
</tbody>
</table>
<p>Eigenvalues of linear ODEs define type of phase plane</p>
<p>The table above summarizes all the different types of flows in the phase plane possible for linear systems, in terms of the behavior of solutions relative to the fixed point at the origin. If the eigenvalues are real, the solutions will be exponential in nature. There are three possibilities for nonzero eigenvalues: <em>stable node</em> (both eigenvalues are negative), <em>unstable node</em> (both eigenvalues are positive), and a <em>saddle point</em> (mixed signs). If one of the eigenvalues is zero, this means that there is not flow along one direction, so there is a <em>line of fixed points</em> in the direction of the corresponding eigenvector (if both
eigenvalues are zero, there is no flow at all.)</p>
<p>For complex eigenvalues, there are three possibilities: if the real part is positive, the solution will grow and oscillate (oscillations with exponentially increasing amplitude), if the real part is negative, the solution will decay and oscillation (oscillations with exponentially increasing amplitude), and if the real part is zero (pure imaginary\ eigenvalues) the solution will oscillate with constant amplitude. The first type is called an <em>unstable spiral</em>, the second a <em>stable spiral</em>, and the third a <em>center</em>. It is not possible for complex eigenvalues of two-dimensional systems to have different signs of real parts, because as the formula shows, the real part is the same for both and is equal to the trace divided by two.</p>
</div>
</div>
<div class="section" id="dynamics-of-romantic-relationships">
<h1><span class="section-number">6.5. </span>Dynamics of romantic relationships<a class="headerlink" href="#dynamics-of-romantic-relationships" title="Permalink to this headline">¶</a></h1>
<p>We examine a model, taken from [&#64;strogatz_nonlinear_2001], that applies dynamical systems modeling to a pressing concern for many humans: the prediction of dynamics of a romantic relationship. There are several unrealistic assumptions involved in the following model: first, that love or affection can be quantified, second, that any changes in relationship depend only on the emotions of the two people involved, and third, that the rate of change of the two love variables depend linearly on each other.</p>
<p>If we can give those assumptions the benefit of the doubt (which is how all relationships begin), we can write down a system of ODEs to describe a romantically involved couple. Here <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dynamic variables that quantify the emotional states of the two lovers:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\
begin{aligned}
\dot  X &amp; = &amp; aX+ bY \\
\dot  Y &amp; = &amp; cX + dY
\end{aligned}
\end{split}\]</div>
<p>Let us denote positive feelings (love) with positive values of <span class="math notranslate nohighlight">\(X,Y\)</span>, while negative values signify negative feelings (hate.) The significance of the parameters can be interpreted as follows: <span class="math notranslate nohighlight">\(a,d\)</span> describe the response of the two people to their own feelings, while <span class="math notranslate nohighlight">\(b,c\)</span> correspond to the effect the other person’s feeling has on their own. For example, a person whose feeling grow as the other person’s affection increases can be modeled with a positive value of <span class="math notranslate nohighlight">\(b\)</span> (or <span class="math notranslate nohighlight">\(c\)</span>). On the other hand, a person whose own feelings are dampened by the other one’s excessively positive emotions, can be decribed by a negative value of <span class="math notranslate nohighlight">\(b\)</span> or <span class="math notranslate nohighlight">\(c\)</span>. Their own feelings can also play a role, either positive or negative, reflected in the sign of the constants <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Using mathematical modeling, we can answer the following basic questions:</p>
<ol class="simple">
<li><p>Given a set of values for parameters <span class="math notranslate nohighlight">\(a, b, c, d\)</span>, predict the  dynamic behavior of the model relationship.</p></li>
<li><p>Find conditions for stability and existence of oscillations in the dynamical system, expressed as a function of the parameters.</p></li>
</ol>
<p>To address the first question, here are some simplified scenarios for our two lovers in the model.</p>
<p><strong>Detached lovers:</strong> Let the emotional state of the two lovers depend only on their own emotions, for example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot  X &amp; = &amp; X \\
\dot  Y &amp; = &amp;  -Y
\end{aligned}
\end{split}\]</div>
<p>To classify the behavior of the model, we find the eigenvalues of the system. In this case, they are the diagonal elements of the matrix, 1 and -1. This mean that the origin is a saddle point, and therefore it is unstable. In the <span class="math notranslate nohighlight">\(X\)</span> direction, the emotions are going to grow without bound, either in the love or hate direction, while in the <span class="math notranslate nohighlight">\(Y\)</span> direction, the emotions are going to decay to zero (indifference). This should be no surprise, that since the two equations are independent, the lovers have no emotional effect on each other.</p>
<p><strong>Lovers with no self-awareness:</strong> Here is an alternate situation: suppose two lovers were not influenced by their own emotions, but were instead attuned to the emotional state of the other. Then we might have the following model, in which lover <span class="math notranslate nohighlight">\(X\)</span> reacts in the opposite way by emotions of lover <span class="math notranslate nohighlight">\(Y\)</span>, but lover <span class="math notranslate nohighlight">\(Y\)</span> is, contrariwise, spurred by the love or hate of <span class="math notranslate nohighlight">\(X\)</span> in the same direction:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\dot  X &amp; = &amp; -Y \\
\dot  Y &amp; = &amp;  X
\end{aligned}
\end{split}\]</div>
<p>We find the eigenvalues of the system by using the expression in equation [eg:2D_eig]: <span class="math notranslate nohighlight">\(\lambda =  (0 \pm \sqrt{-4})/2 = \pm i\)</span>. Pure imaginary eigenvalues tell us that the origin is a center point, with the solutions periodic orbits around the origin. Psychologically, we can interpret this scenario as cycles of love and hate, never growing and never decaying. The magnitude of these oscillations depends on the initial state of the system, that is, the feelings the lovers had at the beginning of the relationship.</p>
<p>We can now address the second question, and find under what circumstances different types of dynamic behaviors occur. We consider the general model, and ask what kinds of eigenvalues are possible for different parameter values. First, we write down the general expression for the eigenvalues, from equation [eg:2D_eig]:</p>
<div class="math notranslate nohighlight">
\[
\lambda =  \frac{a+d \pm \sqrt{(a+d)^2-4(ad-bc)}}{2}
\]</div>
<p>There are two properties we are interested in: stability and existence of oscillations. Recall that stability is determined by the sign of the real part of the eigenvalues. If the square root is imaginary, then the real part is simply the trace (<span class="math notranslate nohighlight">\(a+d\)</span>), but if the square root is real, we have to consider the whole expression to determine stability. So let us first state the condition for existence of oscillations (imaginary square root):</p>
<ol class="simple">
<li><p>Complex eigenvalues: oscillatory solutions <span class="math notranslate nohighlight">\(4(ad-bc) &gt; (a+d)^2\)</span>. If this expression holds, the square root is imaginary, and the stability is determined by the sign of the trace. That is, if <span class="math notranslate nohighlight">\(a+d &gt; 0\)</span>, the system is unstable, and will grow into unbounded love or hate, but if <span class="math notranslate nohighlight">\(a+d &lt; 0\)</span>, then the system is stable, and will spiral to indifference. The special case <span class="math notranslate nohighlight">\(a+d = 0\)</span>, such as we saw above, means that strictly periodic love/hate cycles are the solutions.</p></li>
<li><p>Real eigenvalues: exponential growth and/or decay <span class="math notranslate nohighlight">\(4(ad-bc) &lt; (a+d)^2\)</span>. In this case, the square root is real, and no oscillatory solutions exist. In order to determine whether this implies exponential growth, decay, or a combination, we must weigh the relative sizes of <span class="math notranslate nohighlight">\((a+d)\)</span> and <span class="math notranslate nohighlight">\(\sqrt{(a+d)^2-4(ad-bc)}\)</span>. If <span class="math notranslate nohighlight">\(|a+d| &gt; \sqrt{(a+d)^2-4(ad-bc)}\)</span>, then adding or subtracting the square root does not change the sign of <span class="math notranslate nohighlight">\((a+d)\)</span>: if it is negative, both eigenvalues are negative, and the origin is a stable node, and if the trace is positive, the origin is an unstable node. However, if the absolute value of the root outweighs the absolute value of the trace <span class="math notranslate nohighlight">\(|a+d| &lt; \sqrt{(a+d)^2-4(ad-bc)}\)</span> , then either adding or subtracting the root will change the sign of the eigenvalues. Therefore, one eigenvalue is positive and the other is negative, and the origin is a saddle point. The emotions will run unchecked in some preferred direction, possibly combining love and hate of the two lovers.</p></li>
</ol>
<p>These conditions are not intuitive, and it took some work to express them. The benefit is that now, given any values of the self-involvement parameters <span class="math notranslate nohighlight">\(a,d\)</span> and the sensitivity parameters <span class="math notranslate nohighlight">\(b,c\)</span> we can predict the long-term dynamics of the model relationship. Whether the results have any bearing on reality, of course, depends on how well the reality is described by these primitive assumptions.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="linear_pp_intro.html" title="previous page"><span class="section-number">6. </span>Linear ODEs with two variables</a>
    <a class='right-next' id="next-link" href="linear_pp_python.html" title="next page"><span class="section-number">6.6. </span>Phase portraits in Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dmitry Kondrashov<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>