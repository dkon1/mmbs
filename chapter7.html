

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Forces and potentials in biological modeling &#8212; Mathematical Methods for Biology</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Mathematical Methods for Biology</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter1_intro.html">
   1. Models with one variable in discrete time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter2_intro.html">
   2. Nonlinear discrete-time dynamic models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter3_intro.html">
   3. Discrete models of higher order
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter4_intro.html">
   4. Models with one variable in continuous time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter5_intro.html">
   5. Graphical analysis of ordinary differential equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter6_intro.html">
   6. Classification of 2-variables linear ODEs
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter7.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling-forces-and-simple-springs">
   Modeling: forces and simple springs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exponential-growth-and-decay">
     exponential growth and decay
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#properties-of-linear-oscillations">
     properties of linear oscillations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#potentials-and-forces">
     Potentials and forces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#harmonic-spring-potential">
     Harmonic spring potential
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#two-masses-connected-by-a-spring">
     Two masses connected by a spring
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-eigenvalues-frequencies-and-damping">
   Analytical: eigenvalues, frequencies, and damping
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#converting-second-order-odes-into-first-order">
     Converting second order ODEs into first order
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-behaviors-of-the-harmonic-oscillator">
     dynamic behaviors of the harmonic oscillator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forcing-and-inhomogeneous-odes">
     forcing and inhomogeneous ODEs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forced-oscillations-and-resonance">
     forced oscillations and resonance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analytical-linearity-and-vector-spaces">
   Analytical: linearity and vector spaces
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inner-product-and-orthogonality">
     inner product and orthogonality
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projection-and-decomposition-of-vectors-in-a-basis">
     projection and decomposition of vectors in a basis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-solution-of-linear-odes">
     general solution of linear ODEs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computational-normal-mode-calculations">
   Computational: normal mode calculations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#harmonic-analysis-of-coupled-oscillators">
     harmonic analysis of coupled oscillators
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-mode-calculations">
     normal mode calculations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthesis-normal-mode-analysis-of-biomolecular-structures">
   Synthesis: normal mode analysis of biomolecular structures
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biomolecular-structures-as-elastic-solids">
     biomolecular structures as elastic solids
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sorting-normal-modes-by-frequency">
     sorting normal modes by frequency
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="forces-and-potentials-in-biological-modeling">
<h1>Forces and potentials in biological modeling<a class="headerlink" href="#forces-and-potentials-in-biological-modeling" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In the last chapter we learned to analyze the dynamics of linear
dynamical systems by finding the eigenvalues of corresponding matrices.
In this chapter, we will study a class of mathematical models with a
classical physics pedigree. These models are based on a physical
potential, which is usually given by a physical law, e.g. gravity. These
systems are special, because they have a conserved quantity that is
known as energy. We will learn about the consequences of such conserved
quantities, and what happens when the conservation is broken and energy
dissipates.</p>
<p>The main type of models that we will address are oscillators, with the
simplest being simple springs. These models are in fact relevant for
biological modeling in a variety of fields. We will apply the analysis
to the study of biomolecular flexibility, to predict the preferred
directions of internal motion of parts of protein molecules. The study
of protein structural dynamics has important implication to
understanding the mechanism of function of many biochemical systems. For
example, many signaling molecules undergo conformational changes upon
binding or phosphorylation, which generates changes in their biochemical
actions.</p>
<p>The modeling section explains the basic physics of forces and potential
functions. We will use examples of a single mass on a spring, and the
scale up to two masses connected by a spring. In the analytic section,
we will learn how to turn second order ODEs into first order dynamical
systems, then find the explicit solutions for the oscillations of a
simple spring. We will show how damping enters the solution, and what
effect it has on conservation. We will also learn how to include
external forcing in the model. In the computational section we will
describe a general system of couple linear oscillators using normal mode
analysis. In the synthesis section normal mode analysis is applied to
studying the dynamics of protein structures.</p>
</div>
<div class="section" id="modeling-forces-and-simple-springs">
<h2>Modeling: forces and simple springs<a class="headerlink" href="#modeling-forces-and-simple-springs" title="Permalink to this headline">¶</a></h2>
<p>As we have learned in the last two chapters, linear dynamical systems
have general solutions in the form of a weighted sum of exponentials:
$<span class="math notranslate nohighlight">\(\vec x(t) = \sum_i c_i \vec v_i e^{\lambda_i t}\)</span>$</p>
<p>The type of dynamics possible in a system is determined by the
eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span> of the corresponding matrix. If they are real,
the system will grow or decay exponentially, while the presence of
imaginary part produces oscillations, with growing, decaying, or
constant amplitude, depending on the real part. Below we describe the
modeling situations in which this limited menu of dynamics may be
applied.</p>
<div class="section" id="exponential-growth-and-decay">
<h3>exponential growth and decay<a class="headerlink" href="#exponential-growth-and-decay" title="Permalink to this headline">¶</a></h3>
<p>No biological systems have purely exponential dynamics, as nothing in
nature can realistically grow without bound, or decay inexorably to
zero. Nevertheless, exponential behavior can be useful for modeling the
dynamics in a limited regime, especially near an equilibrium. We have
seen an illustration of this idea in the first part of the course, when
we analyzed nonlinear one dimensional dynamical systems by approximating
them with a linear system near an equilibrium. This linearization
process is also applicable to multidimensional systems, where it
involves the use of a first derivative matrix (called the <em>Jacobian</em>) as
the local linear approximation. We will learn how to do this in detail
in a future chapter, but the idea remains the same: by analyzing the
eigenvalues of the matrix, one can predict whether the solution of the
system grows or decays near the equilibrium. In the first case, the
equilibrium is unstable, in the second, it is stable.</p>
</div>
<div class="section" id="properties-of-linear-oscillations">
<h3>properties of linear oscillations<a class="headerlink" href="#properties-of-linear-oscillations" title="Permalink to this headline">¶</a></h3>
<p>Another area of applicability of linear models is for describing
oscillatory behavior. Many biological systems exhibit oscillatory
dynamics, ranging from heartbeat and circadian rhythms in physiology, to
cycles of biochemical reactions and firing of neurons. We know that
linear systems with complex eigenvalues have oscillatory dynamics, so it
is tempting to describe these phenomena with linear models. As we saw
above, however, linear models cannot describe real biological systems
over all possible values of its variable. Linear oscillations have
specific properties which are generally not found in real systems: if
the real part of the eigenvalue is nonzero, they either have
exponentially growing or decaying amplitudes, which, as we argued above
is not biologically feasible in the long run. This leaves the situation
when the real part is zero, and the oscillations have a constant
amplitude. This situation is not immediately unrealistic, but it has its
own specific limitations: in it, oscillations of all amplitudes are
possible, regardless of the frequency (think of the simple harmonic
oscillator in the previous chapter.) This is also unrealistic, and
nonlinear models are necessary to model the dynamics usually observed in
reality, in which oscillations occurs with a preferred frequency and
amplitude.</p>
</div>
<div class="section" id="potentials-and-forces">
<h3>Potentials and forces<a class="headerlink" href="#potentials-and-forces" title="Permalink to this headline">¶</a></h3>
<p>The dynamics of systems in classical physical are defined by their
potential functions. The <em>potential energy</em> describes the propensity of
the system to do work, that is, to apply <em>force</em> to an object over some
distance. By definition, work is the difference between the potentials
at the two endpoints of the path, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. This can be summarized in
the following equation, relating <em>work</em> <span class="math notranslate nohighlight">\(W\)</span> done on an object by
applying force <span class="math notranslate nohighlight">\(f(x)\)</span> over the distance from <span class="math notranslate nohighlight">\(a\)</span> to <span class="math notranslate nohighlight">\(b\)</span>:
$<span class="math notranslate nohighlight">\(W = V(b) - V(a) = \int_a^b f(x) dx\)</span><span class="math notranslate nohighlight">\( Intuitively, one can think of a
potential function \)</span>V(x)$ as a law handed from on high, which dictates
what forces are going to act on the system. The forces then make the
objects in the systems move, generating dynamics that we care about and
will investigate in this chapter.</p>
<p>The above connection between potential and force has a familiar form. In
fact, it is equivalent to the Fundamental Theorem of calculus, where we
define the force <span class="math notranslate nohighlight">\(f(x)\)</span> to be the derivative of the potential <span class="math notranslate nohighlight">\(V(x)\)</span>.
This relationship is at the center of this chapter: $<span class="math notranslate nohighlight">\(f(x) = -V'(x)
\label{eq:force_pot}\)</span>$ Graphically speaking, the force is the negative
slope of the potential function. This means that if the force is always
pushing the object down the slope: if the potential is rising, it is
pushing backward, and if the potential is falling, it is pushing
forward. Essentially, this definition is consistent with the metaphor of
potential as a terrain, with gravitational force bringing objects down
to the lowest point of the landscape.</p>
</div>
<div class="section" id="harmonic-spring-potential">
<h3>Harmonic spring potential<a class="headerlink" href="#harmonic-spring-potential" title="Permalink to this headline">¶</a></h3>
<p>We will define a specific potential for a mass on a spring and
investigate the resulting dynamical system in the analytical section.
The assumptions are: a) that there is a position <span class="math notranslate nohighlight">\(x_0\)</span> at which the
spring is at rest, that is, no force is acting on the object, and b)
that the force will push the object back toward the resting position,
with strength proportional to the displacement <span class="math notranslate nohighlight">\((x-x_0)\)</span> of the mass
from the resting position. Turns out that this model is defined by the
following potential function: $<span class="math notranslate nohighlight">\(V(x) = \frac{1}{2}k(x-x_0)^2\)</span><span class="math notranslate nohighlight">\( Here \)</span>k<span class="math notranslate nohighlight">\(
is known as the *spring constant*, and this parameter describes the
strength of the restoring force: for large \)</span>k<span class="math notranslate nohighlight">\(, the mass will be pulled
toward the resting state with greater force than with a smaller \)</span>k<span class="math notranslate nohighlight">\(.
Notice that the potential has a minimum at \)</span>x_0$, since this is the
position that is most favorable for the system. The variables and
parameters of the model are illustrated in figure [fig:harm_osci].</p>
<p><img alt="Mass on a spring, with a restoring force  and rest position .http://openlearn.open.ac.uk/[]{data-label=&quot;fig:harm_osci&quot;}" src="fig_ch7/harmonic_oscillator.jpg" />{width=”2.5in”}</p>
<p>Using the relationship in equation [eq:force_pot] above, we conclude
that the <em>restoring force</em> in the system obeys the following equation:
$<span class="math notranslate nohighlight">\(f(x) = k (x_0-x)\)</span>$ This model of a simple spring with a quadratic
potential and linear force is called a <em>Hookean</em> spring model, after the
physicist Robert Hooke. The idealized linear spring model is also called
a <em>harmonic oscillator</em>, because, as we will see in the analytical
section, its solutions are perfect periodic oscillations.</p>
<p>The expression for force can be translated into the language of ODEs and
dynamics using Newton’s second law, which states that <span class="math notranslate nohighlight">\(f = ma\)</span>. Recall
that the acceleration in physics is the second derivative of position,
and we can write down the dynamical system of a harmonic oscillator as
follows: $<span class="math notranslate nohighlight">\(m\ddot x = k (x_0-x)\)</span>$ We will learn to solve this equation
in the next section.</p>
<p>In reality, there is usually another force that acts to slow down any
moving object. This effect is called <em>kinetic friction</em>, or <em>viscosity</em>
if the object is in a fluid, such as air or water. The typical model for
kinetic friction assumes the friction force is proportional to the
velocity of the object. The relationship has a negative sign, since the
force acts in the opposite direction of the velocity, slowing down the
motion. The friction force <span class="math notranslate nohighlight">\(g(x)\)</span> is defined as follows.
$<span class="math notranslate nohighlight">\(g(x) = - \gamma v = - \gamma \dot x\)</span><span class="math notranslate nohighlight">\( Thus, a system incorporating a
harmonic oscillator with friction has the following equation of motion:
\)</span><span class="math notranslate nohighlight">\(m\ddot x = k (x_0-x) - \gamma \dot x\)</span>$</p>
</div>
<div class="section" id="two-masses-connected-by-a-spring">
<h3>Two masses connected by a spring<a class="headerlink" href="#two-masses-connected-by-a-spring" title="Permalink to this headline">¶</a></h3>
<p>We have only looked at a single oscillator, whose dynamics depend solely
on its own position. Now, consider two separate objects connected by a
linear Hookean spring. Let us define <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> to be the displacements
of the two objects from the respective resting positions; this way we
don’t have to keep around <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(y_0\)</span>. The force depends on the
distance between the two of them, and if we restrict the model to one
dimension, it depends on the difference between the two displacements.
The force on each particle is in the opposite direction of its own
displacement and in the same direction as the other particle’s
displacement, so the equations are: $<span class="math notranslate nohighlight">\(\begin{aligned}
m \ddot x &amp;=&amp; -k(x-y) \\
m \ddot y &amp;=&amp; -k(y-x)\end{aligned}\)</span><span class="math notranslate nohighlight">\( The dynamical system can be
expressed in matrix form, where the matrix \)</span>A<span class="math notranslate nohighlight">\( is:
\)</span><span class="math notranslate nohighlight">\(A = \frac{k}{m}\left(\begin{array}{cc}-1 &amp; 1 \\1 &amp; -1\end{array}\right)\)</span><span class="math notranslate nohighlight">\(
Then the system of ODEs can be written down more concisely, with the
vector \)</span>\vec x<span class="math notranslate nohighlight">\( contains the positions of both \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\ddot {\vec x} = A \vec x\)</span>$</p>
</div>
</div>
<div class="section" id="analytical-eigenvalues-frequencies-and-damping">
<h2>Analytical: eigenvalues, frequencies, and damping<a class="headerlink" href="#analytical-eigenvalues-frequencies-and-damping" title="Permalink to this headline">¶</a></h2>
<div class="section" id="converting-second-order-odes-into-first-order">
<h3>Converting second order ODEs into first order<a class="headerlink" href="#converting-second-order-odes-into-first-order" title="Permalink to this headline">¶</a></h3>
<p>In the previous section we found the can transform it into a 2D system
and use the geometric methods of phase plane analysis analysis. Let us
introduce a second variable <span class="math notranslate nohighlight">\(y = \dot x\)</span>. Then <span class="math notranslate nohighlight">\( \dot y = \ddot x\)</span>, and
we can write a system of equations in the standard first order form,
which will enable us to use our analytical tools.</p>
<p><strong>Example.</strong> Let us consider the harmonic oscillator with no damping, as
defined in the modeling section. The second order equation can be
written as two first order equations: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot y &amp;=&amp; - \frac{k}{m}x \\
\dot x &amp;=&amp; y\end{aligned}\)</span><span class="math notranslate nohighlight">\( The eigenvalues of this system are
\)</span>\lambda = \pm \sqrt\frac{k}{m} i<span class="math notranslate nohighlight">\(, and \)</span>\sqrt\frac{k}{m}<span class="math notranslate nohighlight">\( is the
frequency of oscillation of this model, which in physics is used to
model a simple Hookean spring. The solutions are in the form of sines
and cosines, and we can write it:
\)</span><span class="math notranslate nohighlight">\(x(t) = A \cos(\sqrt\frac{k}{m}t) + B \sin(\sqrt\frac{k}{m}t)\)</span><span class="math notranslate nohighlight">\( The
constants \)</span>A<span class="math notranslate nohighlight">\( and \)</span>B<span class="math notranslate nohighlight">\( are determined by the initial conditions. Note
that for second order equations, two initial conditions must be
specified (e.g. one for \)</span>x<span class="math notranslate nohighlight">\( and one for \)</span>\dot x$) to determine the two
integration constants.</p>
<p><strong>Example.</strong> Let us now consider the harmonic oscillator with damping
added. The second order equation can be written as two first order
equations: $<span class="math notranslate nohighlight">\(\begin{aligned}
\dot y &amp;=&amp; - \frac{k}{m}x -\frac{\gamma}{m}y\\
\dot x &amp;=&amp; y\end{aligned}\)</span><span class="math notranslate nohighlight">\( The eigenvalues of this system are:
\)</span>\lambda = (-\gamma \pm \sqrt{\gamma^2-4km})/2m<span class="math notranslate nohighlight">\(. This means, depending
on the sign of \)</span>\gamma^2-4km<span class="math notranslate nohighlight">\(, the eigenvalues may be either real or
complex. If they are complex, the solutions will be damped oscillations
because the real part (\)</span>-\gamma<span class="math notranslate nohighlight">\() is negative. The solution can be
written with the following exponential and oscillation terms:
\)</span><span class="math notranslate nohighlight">\(x(t) = Ae^{- \frac{\gamma}{m} t}\cos [(\sqrt{k/m-\gamma^2/4m^2}) t ]+Be^{- \frac{\gamma}{m} t}\sin [ (\sqrt{k/m-\gamma^2/4m^2}) t]\)</span>$</p>
</div>
<div class="section" id="dynamic-behaviors-of-the-harmonic-oscillator">
<h3>dynamic behaviors of the harmonic oscillator<a class="headerlink" href="#dynamic-behaviors-of-the-harmonic-oscillator" title="Permalink to this headline">¶</a></h3>
<p>We have seen that when there is a restoring force <span class="math notranslate nohighlight">\(F = -kx\)</span>, were <span class="math notranslate nohighlight">\(x\)</span> is
the displacement from a resting value, the differential equation from
Newton’s second law is <span class="math notranslate nohighlight">\(m \ddot x = - kx\)</span>, or <span class="math notranslate nohighlight">\(\ddot x = -\omega^2 x\)</span>,
with <span class="math notranslate nohighlight">\(\omega = \sqrt{k/m}\)</span>. We can see immediately from this expression
that sines and cosines are solutions, because by taking two derivatives
one gets back the same function, multiplied by the negative square of
the frequency (check this yourself). This is also what we get from
eigenvalue analysis, by finding the eigenvalues of the system
<span class="math notranslate nohighlight">\(\pm \omega i\)</span>.</p>
<p>If there is a nonzero first derivative term in the force,
<span class="math notranslate nohighlight">\(F = -kx + \gamma \dot x\)</span>; if <span class="math notranslate nohighlight">\(\gamma\)</span> is negative it is called a
damping term. Let us analyze this scenario by finding the eigenvalues of
the corresponding second-order ODE
<span class="math notranslate nohighlight">\(\ddot x + \gamma/m \dot x + \omega^2 x = 0\)</span>:
$<span class="math notranslate nohighlight">\(\lambda = (-\gamma/m \pm \sqrt{(\gamma/m)^2 - 4\omega^2})/2\)</span><span class="math notranslate nohighlight">\( We see
that the eigenvalues are real if \)</span>(\gamma/m)^2 &gt; 4\omega^2<span class="math notranslate nohighlight">\(, and complex
if \)</span>(\gamma/m)^2 &lt; 4\omega^2<span class="math notranslate nohighlight">\(, and the solutions would be decaying
exponentials. Physically speaking, if damping is too strong, the
system’s propensity for oscillations is overpowered, and the
displacement \)</span>x<span class="math notranslate nohighlight">\( never gets to the other side of 0 (think of a mass on a
spring in molasses). This system is called *overdamped*. On the other
hand, if \)</span>\gamma<span class="math notranslate nohighlight">\( is below the threshold value, oscillations persist,
although they tend to zero with time, in the form of
\)</span>e^{-\gamma/m}(A\sin(\omega t) + B\cos(\omega t))<span class="math notranslate nohighlight">\(. This system is
called *underdamped*. At \)</span>(\gamma/m)^2 = 4\omega^2$, the situation is
called <em>critical damping</em> - the oscillator will return to its resting
state and just miss overshooting it.</p>
<p>If <span class="math notranslate nohighlight">\(\gamma\)</span> were positive (I cannot think of a physical or biological
example) the situation would be reversed, leading to exponentially
growing oscillations or pure exponential growth.</p>
</div>
<div class="section" id="forcing-and-inhomogeneous-odes">
<h3>forcing and inhomogeneous ODEs<a class="headerlink" href="#forcing-and-inhomogeneous-odes" title="Permalink to this headline">¶</a></h3>
<p>We will now consider a system that consists of a harmonic oscillator to
which which an external force is applied. Mathematically, the force is
external if it does not depend on the variables of the system, although
it may depend on time.</p>
<p>Now let us consider a harmonic oscillator that is driven by a periodic
external force. This could represent a mass on a spring which is being
“forced” by an external influence, or an oscillating neuron that
receives a periodic signal from another neuron.</p>
<p>We will now consider an inhomogeneous ODE with the <em>method of
undetermined coefficients</em>. The rule of thumb is: if the form of the
inhomogeneity is unchanged by differentiation (e.g. exponentials,
polynomials) then use this form multiplied by some constants as a guess
for the particular solution. Then substitute it into the ODE, and find
which values of the constants will satisfy the ODE.</p>
<div class="math notranslate nohighlight">
\[\ddot x + \omega_0^2x = C\cos(\omega t)$$ First, the solution of the
homogeneous equation is a sum of sines and cosines with frequency
$\omega_0$: $x_h =A \cos(\omega_0t) + B\sin(\omega_0t) $. To find the
particular solution, let us assume that the solution has the same form
as the forcing term, with some undetermined coefficients:
$x_h = C_1\cos(\omega t) + C_2 \sin(\omega t)$. The second derivative is
then:
$\ddot x_h = -C_1\omega^2\cos(\omega t) -C_2 \omega^2\sin(\omega t)$.
Plugging this into the equation we have:
$$-C_1\omega^2\cos(\omega t) -C_2 \omega^2\sin(\omega t) +  \omega_0^2( C_1\cos(\omega t) + C_2 \sin(\omega t)) = C\cos(\omega t)\]</div>
<p>Since there is no sine term on the right hand side, we need to set
<span class="math notranslate nohighlight">\(C_2 = 0\)</span>. The cosine terms yield the following expression:
<span class="math notranslate nohighlight">\((-C_1\omega^2 + \omega_0^2 C_1)\cos(\omega t) = C\cos(\omega t) \Rightarrow C_1 = C/( \omega_0^2-\omega^2)\)</span>.
Adding the homogeneous and the particular solutions together, we find
the general solution for a harmonic oscillator with a periodic driving
force:
$<span class="math notranslate nohighlight">\(x(t) = x_h + x_p =  A \cos(\omega_0t) + B\sin(\omega_0t) + \frac{C}{\omega_0^2-\omega^2}\cos(\omega t)\)</span><span class="math notranslate nohighlight">\(
The solution is a superposition of oscillations at the inherent
frequency of the oscillator (\)</span>\omega_0<span class="math notranslate nohighlight">\() and the external driving
frequency (\)</span>\omega$). What happens when the two frequencies match?</p>
</div>
<div class="section" id="forced-oscillations-and-resonance">
<h3>forced oscillations and resonance<a class="headerlink" href="#forced-oscillations-and-resonance" title="Permalink to this headline">¶</a></h3>
<p>When <span class="math notranslate nohighlight">\(\omega = \omega_0\)</span> he particular solution found above no longer
exists because of division by zero. Thus, we need to seek another
solution. Let us try the guess of
<span class="math notranslate nohighlight">\(x_h = C_1t\cos(\omega t) + C_2 t\sin(\omega t)\)</span>. Let us find its
derivative, using the product rule:
<span class="math notranslate nohighlight">\( \dot x_h = C_1\cos(\omega t) - C_1\omega t\sin(\omega t) + C_2 \sin(\omega t) + C_2 \omega  t\cos(\omega t)\)</span>.
The second derivative then becomes:
<span class="math notranslate nohighlight">\(\ddot x_h = -C_1 \omega  \sin(\omega t) - C_1 \omega \sin(\omega t) - C_1\omega^2 t\cos(\omega t)  + C_2 \omega  \cos(\omega t) + C_2\omega  \cos(\omega t) -C_2\omega^2  t\sin(\omega t)\)</span>.
Substituting this into the inhomogeneous ODE, we have:
$<span class="math notranslate nohighlight">\(-2C_1\omega \sin(\omega t) - C_1\omega^2 t\cos(\omega t)  + 2C_2 \omega \cos(\omega t) -C_2\omega ^2 t\sin(\omega t)  + \omega^2( C_1t\cos(\omega t) + C_2t \sin(\omega t)) = C\cos(\omega t)\)</span><span class="math notranslate nohighlight">\(
Let us break this up into equations for the different terms:
\)</span><span class="math notranslate nohighlight">\(-2C_1\omega \sin(\omega t)  = 0\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(-2C_2\omega \cos(\omega t) = C\cos(\omega t)\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(- C_1\omega^2 t\cos(\omega t) + C_1 \omega^2 t\cos(\omega t) = 0\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(-C_2\omega ^2 t\sin(\omega t) + C_2 \omega^2 t \sin(\omega t) =0\)</span><span class="math notranslate nohighlight">\(
Note that the latter two are true for any values of the constants. The
first one requires that \)</span>C_1 = 0<span class="math notranslate nohighlight">\(, and the second one gives
\)</span>C_2 = -C/2<span class="math notranslate nohighlight">\(. Thus, the particular solution is:
\)</span><span class="math notranslate nohighlight">\(x_p(t) = -\frac{C}{2}t \sin(\omega t)\)</span>$ Driving an undamped harmonic
oscillator at its natural frequency results in linearly growing,
unbounded oscillations. This phenomenon is called <em>resonance</em>, and
although no natural system can exhibit unbounded growth in oscillations,
resonance is a profound natural phenomenon, resulting in physical
effects such as collapses of bridges if an external force (e.g. wind)
happens to match its resonant frequency, or in more useful applications,
giving us amplification of radio signals by resonant circuits, as well
as sophisticated biological mechanisms that we will discuss later.</p>
</div>
</div>
<div class="section" id="analytical-linearity-and-vector-spaces">
<h2>Analytical: linearity and vector spaces<a class="headerlink" href="#analytical-linearity-and-vector-spaces" title="Permalink to this headline">¶</a></h2>
<p>In this section we will expand our analysis of linear systems to sketch
a broad picture of linear algebra and its fundamental concepts. For a
more thorough exposition, see [&#64;strang_linear_2005]. Whenever we deal
with more than one variable, they can be concisely written as a vector
of multiple dimensions. We have seen that equations defining linear
dynamical systems can be expressed as products of matrices and vectors.
In order to understand how these systems operate and how to express
their general solutions, we first need to be specific about the notions
of linearity and how it affects vector spaces.</p>
<p>The nomenclature of linearity is derived from the functional description
of a line in the plane. Any line passing through the origin can be
described as a set of points that can be generated by multiplying them
by a single scalar, called the slope, that is, it is generated by a
linear transformation <span class="math notranslate nohighlight">\(f(x) = ax\)</span>. This concept is generalized from
dealing with scalars to vectors by the following defintion:</p>
<p>A <em>linear transformation</em> or <em>linear operator</em> is a mapping <span class="math notranslate nohighlight">\(L\)</span> between
two sets of vectors with the following properties:</p>
<ol class="simple">
<li><p><em>(scalar multiplication)</em> <span class="math notranslate nohighlight">\(L(c \vec v) = c L(\vec v)\)</span>; where <span class="math notranslate nohighlight">\(c\)</span> is
a scalar and <span class="math notranslate nohighlight">\(\vec v\)</span> is a vector</p></li>
<li><p><em>(additive)</em> <span class="math notranslate nohighlight">\(L(\vec v_1 + \vec v_2) =  L(\vec v_1) + L(\vec v_2)\)</span>;
where <span class="math notranslate nohighlight">\(\vec v_1\)</span> and <span class="math notranslate nohighlight">\(\vec v_2\)</span> are vectors</p></li>
</ol>
<p>We have already seen examples of linear transformations, in the form of
matrices multiplying a vector. Matrix multiplication shares the linear
property with scalar multiplication, but it transforms vectors to
vectors, depending on the size of the matrix, and has more complicated
properties. The notion of linearity then leads to the important idea of
combining different vectors:</p>
<p>A <em>linear combination</em> of <span class="math notranslate nohighlight">\(n\)</span> vectors <span class="math notranslate nohighlight">\(\{ \vec v_i \}\)</span> is a weighted sum
of these vectors with any real numbers <span class="math notranslate nohighlight">\(\{a_i\}\)</span>:
$<span class="math notranslate nohighlight">\(a_1 \vec v_1+ a_2 \vec v_2... + a_n \vec v_n\)</span>$</p>
<p>Linear combinations arise naturally from the notion of linearity,
combining the additive property and the scalar multiplication property.
Speaking intuitively, a linear combination of vectors produces a new
vector that is related to the original set. Linear combinations give a
simple way of generating new vectors, and thus invite the following
definition for a collection of vectors closed under linear combinations:</p>
<p>A <em>vector space</em> is a collection of vectors such that a linear
combination of any <span class="math notranslate nohighlight">\(n\)</span> vectors (with <span class="math notranslate nohighlight">\(n \in \mathbb{N}\)</span>) is contained in
the vector space.</p>
<p>The most common examples are the spaces of all real-valued vectors of
dimension <span class="math notranslate nohighlight">\(n\)</span>, which are denoted by <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. For instance,
<span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> (pronounced “r two”) is the vector space of two
dimensional real-valued vectors such as <span class="math notranslate nohighlight">\((1,3)\)</span> and <span class="math notranslate nohighlight">\((\pi, -\sqrt{17})\)</span>;
similarly, <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> is the vector space consisting of three
dimensional real-valued vectors such as <span class="math notranslate nohighlight">\((0.1,0,-5.6432)\)</span>. You can
convince yourself, by taking linear combinations of vectors, that these
vector spaces contain all the points in the usual Euclidean plane and
three-dimensional space. The real number line can also be thought of as
the vector space <span class="math notranslate nohighlight">\(\mathbb{R}^1\)</span>.</p>
<p>How can we describe a vector space, without trying to list all of its
elements? We know that one can generate an element by taking linear
combinations of vectors. It turns out that it is possible to generate
(or “span”) a vector space by taking linear combinations of a subset of
its vectors. The challenge is to find a miminal subset of subset that is
not redundant. In order to do this, we first introduce a new concept:</p>
<p>A set of vectors <span class="math notranslate nohighlight">\(\{ \vec v_i \}\)</span> is called <em>linearly independent</em> if
the only linear combination involving them that equals the zero vector
is if all the coefficients are zero.
(<span class="math notranslate nohighlight">\(a_1 \vec v_1+ a_2 \vec v_2... + a_n \vec v_n = 0 \)</span> only if <span class="math notranslate nohighlight">\(a_i = 0\)</span>
for all <span class="math notranslate nohighlight">\(i\)</span>.)</p>
<p>In the familiar Euclidean spaces, e.g. <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, linear
independence has a geometric meaning: two vectors are linearly
independent if the segments from the origin to the endpoint do not lie
on the same line. But it can be shown that any set of three vectors in
the plane is linearly <em>dependent</em>, because there are only two dimensions
in the vector space. This brings us to the key definition of this
section:</p>
<p>A <em>basis</em> of a vector space is a linearly independent set of vectors
that generate (or span) the vector space.</p>
<p>A vector space generally has many possible bases, as illustrated in
figure [fig:2bases]. In the case of <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, the canonical
basis set is <span class="math notranslate nohighlight">\(\{(1,0); (0,1)\}\)</span> which obviously generates any point on
the plane and is linearly independent. But any two linearly independent
vectors can generate any vector in the plane. For instance, the vector
<span class="math notranslate nohighlight">\(\vec r = (2,1)\)</span> can be represented as a linear combination of the two
canonical vectors: <span class="math notranslate nohighlight">\(\vec r = 2(1,0)+(0,1)\)</span>. Let us choose another basis
set, by taking the vector itself as one of the basis vectors and leaving
the second one from the canonical basis: <span class="math notranslate nohighlight">\(\{(2,1); (0,1)\}\)</span> The same
vector can be represented by a linear combination of these two vectors,
with coefficients 1 and 0: <span class="math notranslate nohighlight">\(\vec r = 1 (2,1) + 0(0,1)\)</span>. Since multiple
bases are possible, we need a way of evaluating them and changing
between them.</p>
<p><img alt="Two basis sets in the plane. Green arrows show the canonical cartesianbasis, while the red arrows correspond to the basis set. Any point in the plane can be described in termsof both bases. http://www.math.hmc.edu/calculus/tutorials/changebasis/[]{data-label=&quot;fig:2bases&quot;}" src="fig_ch8/2bases.png" />{width=”3in”}</p>
<div class="section" id="inner-product-and-orthogonality">
<h3>inner product and orthogonality<a class="headerlink" href="#inner-product-and-orthogonality" title="Permalink to this headline">¶</a></h3>
<p>Not all basis sets are created equal. Continuing our geometric analogy,
the canonical basis in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> is related to the familiar
Cartesian coordinates, with two orthogonal axes in the direction of the
basis vectors <span class="math notranslate nohighlight">\(\{(1,0); (0,1)\}\)</span> There are many other, non-orthogonal
bases, like <span class="math notranslate nohighlight">\(\{(2,1); (0,1)\}\)</span> above, but they are intuitively less
economical, since one vector can make a contribution in the direction of
the other. This means that a given vector can be represented in
non-unique linear combinations with a non-orthogonal basis set. Thus,
mathematicians prefer what are called orthogonal bases. In order to
define orthogonality, we first introduce this key notion:</p>
<p>The <em>inner product</em> of two vectors <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> is defined as the product
of <span class="math notranslate nohighlight">\(u\)</span> and the conjugate transpose <span class="math notranslate nohighlight">\(v^*\)</span>:
<span class="math notranslate nohighlight">\( \langle u,v \rangle  = uv^*\)</span>.</p>
<p>This is the general definition of inner product, rather than the more
familiar notion of the dot product, which only applies to real vector
spaces. One important difference that the inner product in this
definition has a modified commutative property:
<span class="math notranslate nohighlight">\(\langle u,v \rangle = \overline{\langle v,u \rangle}\)</span> - meaning that
switching the order of the inner product results in a complex conjugate
of the result. For example: <span class="math notranslate nohighlight">\(u=(-i, 5+i)\)</span> and <span class="math notranslate nohighlight">\(v=(6, 4-3i)\)</span>.
<span class="math notranslate nohighlight">\(\langle u,v \rangle = -i*6 + (5+i)*(4+3i) = -6i+20-3+19i= 17+13i\)</span>, and
<span class="math notranslate nohighlight">\(\langle v,u \rangle = 6i+(4-3i)*(5-i) = 6i+20 -3- 15i-4i = 17 - 13i\)</span>.</p>
<p>The inner product is intimately tied to the geometric notion of
direction of vectors in the Euclidean spaces. Let us consider a pair of
vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> of unit length. If the two vectors have the
same direction (are <em>colinear</em>), their inner product is equal to 1 or
-1, depending on whether they are parallel or anti-parallel - for
example, consider the cases of <span class="math notranslate nohighlight">\(\langle (1,0), (1,0) \rangle = 1\)</span> and
<span class="math notranslate nohighlight">\(\langle (0,1),(0,-1) \rangle = -1\)</span>. If we rotate one vector relative to
the other - for instance, keep the first vector fixed at <span class="math notranslate nohighlight">\((1,0)\)</span> and let
the other one be <span class="math notranslate nohighlight">\((a,b)\)</span>, with the restriction that <span class="math notranslate nohighlight">\(\sqrt{a^2+b^2} =1\)</span>,
that is, it is of length 1. Their inner product is clearly equal to <span class="math notranslate nohighlight">\(a\)</span>,
which you should be able to convince yourself, is the cosine of the
angle between the two vectors. With a little bit more work, you can
demonstrate that this statement is true for any two unit vectors (those
with length 1.)</p>
<p>What about vectors of arbitrary length? First, let us define the notion
of length:</p>
<p>The <em>length</em> (also known as the <em>norm</em>) of a vector <span class="math notranslate nohighlight">\(u\)</span> is defined as
the square root of the inner product with itself
<span class="math notranslate nohighlight">\( ||u|| = \sqrt { \langle u,u \rangle } = \sqrt{ uu^*}\)</span>.</p>
<p>For Euclidean vector spaces, this definition agrees with the familiar
Euclidean distance, e.g. in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, for <span class="math notranslate nohighlight">\(\vec v = (x,y)\)</span>,
<span class="math notranslate nohighlight">\(||\vec v|| = \sqrt{x^2+y^2}\)</span>.</p>
<p>Using the notion of length, or norm, vectors can be <em>normalized</em>, which
means divided by the norm, creating a vector of length 1 (a.k.a. unit
vector) with the same direction as the original. Since we know that the
cosine of the angle <span class="math notranslate nohighlight">\(\theta\)</span> between the vectors is the inner product of
two unit vectors, we have the following relationship between any two
vectors:
$<span class="math notranslate nohighlight">\(\left\langle  \frac{\vec v}{||\vec v||} , \frac{\vec u}{||\vec u||} \right \rangle  = \cos(\theta) \Rightarrow \left\langle  \vec v, \vec u\right \rangle  =||\vec v|| ||\vec u||\cos(\theta)\)</span>$</p>
<p>Finally, this leads us to the general statement about orthogonality in
vector spaces, which is crucial not only for regular finite-dimensional
vector spaces, but also in infinite-dimensional function spaces which we
will see later:</p>
<p>Two vectors are <em>orthogonal</em> if their inner product is zero.</p>
<p>Now that we know how to determine whether two vectors are orthogonal, we
will call a basis set orthogonal if all pairs of vectors in it are
orthogonal. Furthermore, it is typically convenient to require that all
the vectors be of unit length, which can be accomplished by
normalization. A basis set of mutually orthogonal unit vectors is called
<em>orthonormal</em>. Typical examples are Cartesian coordinate vectors, such
as <span class="math notranslate nohighlight">\(\{ (1,0,0); (0,1,0); (0,0,1) \}\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>.</p>
</div>
<div class="section" id="projection-and-decomposition-of-vectors-in-a-basis">
<h3>projection and decomposition of vectors in a basis<a class="headerlink" href="#projection-and-decomposition-of-vectors-in-a-basis" title="Permalink to this headline">¶</a></h3>
<p>The basis set of a vector space serves as its defining structure, like a
skeleton giving shape to the gelatinous multitude of vectors. Any
element in a vector space can be described as a linear combination of
the basis set. In the Euclidean plane, the vector <span class="math notranslate nohighlight">\((3,4)\)</span> can be either
thought of a collection of two numbers, or as a point with coordinates
<span class="math notranslate nohighlight">\(x=3\)</span> and <span class="math notranslate nohighlight">\(y=4\)</span>. The latter concept refers to the linear combination of
the two standard basis vectors with <em>coefficients</em> 3 and 4:
<span class="math notranslate nohighlight">\((3,4) = 3(1,0) + 4(0,1)\)</span>. The coefficients quantify the overlap of a
vector <span class="math notranslate nohighlight">\(\vec r\)</span> in question with each of the respective basis vectors.
Geometrically, if a vector is parallel to a basis vector (of unit
length,) then it can be represented as a multiple of the basis vector,
and the coefficient will be equal to the length of the vector <span class="math notranslate nohighlight">\(\vec r\)</span>.
On the other hand, if a basis vector is orthogonal to the vector
<span class="math notranslate nohighlight">\(\vec r\)</span>, the corresponding coefficient will be 0.</p>
<p>The representation of an arbitrary vector of a vector space as a linear
combination of a given basis set is called the <em>decomposition</em> of the
vector in terms of the basis. However, we saw that many possible bases
exist for any vector space. Even if we choose only orthonormal bases,
there are many possibilities: for instance, in the space of real two
dimensional vectors <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>, the standard basis
<span class="math notranslate nohighlight">\(\{(1,0); (0,1)\}\)</span> can be rotated to produce a different orthonormal
basis, e.g. <span class="math notranslate nohighlight">\(\{ (1/\sqrt 2, 1/\sqrt 2); \; (-1/\sqrt 2, 1/\sqrt 2) \}\)</span>.
Therefore, the choice of a basis determines how a given vector is
represented. The decomposition of a vector in terms of a particular
basis is very useful in high-dimensional spaces, where a clever choice
of a basis can allow a description of a vector in terms of contributions
of only a few basis vectors. The vector may then be represented, given
the basis set, with a few coefficients of the relevant basis vectors.</p>
<p><img alt="Projection of vector  onto vector , with angle  betweentheir directions.http://en.wikipedia.org/wiki/Vector_projection[]{data-label=&quot;fig:proj&quot;}" src="fig_ch8/vec_proj.png" />{width=”3in”}</p>
<p>To obtain the coefficients of the basis vectors in a decomposition of a
vector <span class="math notranslate nohighlight">\(\vec r\)</span>, we need to perform what is termed a <em>projection</em> of the
vector onto the basis vectors. Think of shining a light perpendicular to
the basis vector, and measuring the length of the shadow cast by the
vector <span class="math notranslate nohighlight">\(\vec r\)</span> onto <span class="math notranslate nohighlight">\(\vec v_i\)</span>. If the vectors are parallel, the shadow
is equal to the length of <span class="math notranslate nohighlight">\(\vec r\)</span>; if they are orthogonal, the shadow
is nonexistent. To find the length of the shadow, use the inner product
of <span class="math notranslate nohighlight">\(\vec r\)</span> and <span class="math notranslate nohighlight">\(\vec v\)</span>, which as you recall corresponds to the cosine
of the angle between the two vectors multiplied by their norms:
<span class="math notranslate nohighlight">\(\left\langle  \vec r, \vec v\right \rangle  =||\vec r|| ||\vec v||\cos(\theta) \)</span>
(see figure [fig:proj].) We do not care about the length of the vector
<span class="math notranslate nohighlight">\(\vec v\)</span> we are projecting onto, thus we divide the inner by product by
the square norm of <span class="math notranslate nohighlight">\(\vec v\)</span>, and then multiply the vector <span class="math notranslate nohighlight">\(\vec v\)</span> by
this projection coefficient:
$<span class="math notranslate nohighlight">\(Proj(\vec r ; \vec v) = \frac{ \langle \vec r , \vec v \rangle  } {\langle \vec v , \vec v \rangle } \vec v = \frac{ \langle \vec r ,  \vec v \rangle  } {|| \vec v ||^2} \vec v= \frac{  ||\vec r|| \cos(\theta) } {|| \vec v ||}\vec v\)</span><span class="math notranslate nohighlight">\(
This formula gives the projection of the vector \)</span>\vec r<span class="math notranslate nohighlight">\( onto \)</span>\vec v<span class="math notranslate nohighlight">\(,
the result is a new vector in the direction of \)</span>\vec v<span class="math notranslate nohighlight">\(, with the scalar
coefficient \)</span>a = \ \langle \vec r ,  \vec v \rangle  /|| \vec v ||^2$.</p>
<p><strong>Example:</strong> Let us decompose the vector <span class="math notranslate nohighlight">\((3,4)\)</span> in a nonstandard basis,
e.g. the canonical basis rotated by <span class="math notranslate nohighlight">\(45^\circ\)</span>:
<span class="math notranslate nohighlight">\(\{ (1/\sqrt 2, 1/\sqrt 2); \; (-1/\sqrt 2, 1/\sqrt 2) \}\)</span>. Use the
projection formula above to obtain the coefficients of decomposition.
The length of both basis vectors is 1, so the denominator is unity:
$<span class="math notranslate nohighlight">\(a_1 = (\frac{1}{\sqrt 2}, \frac{1}{\sqrt 2}) \cdot (3,4) = \frac{7}{\sqrt 2}; \; a_2 = (-\frac{1}{\sqrt 2}, \frac{1}{\sqrt 2}) \cdot (3,4) = \frac{1}{\sqrt 2}\)</span><span class="math notranslate nohighlight">\(
Therefore, we have the following decomposition:
\)</span><span class="math notranslate nohighlight">\((3,4) = \frac{7}{\sqrt 2} (\frac{1}{\sqrt 2}, \frac{1}{\sqrt 2})  + \frac{1}{\sqrt 2} (-\frac{1}{\sqrt 2}, \frac{1}{\sqrt 2})\)</span>$</p>
<p>Why go through this rigmarole of expressing one vector in terms of <span class="math notranslate nohighlight">\(N\)</span>
others? A decomposition in terms of a basis is necessary to express a
vector in any vector space, as the basis serves as a coordinate system
and the coefficients as coordinates of the point designated by the
vector. Even in the regular Euclidean space, a vector (e.g. <span class="math notranslate nohighlight">\((3,4)\)</span>)
requires an implicit basis set in order to make it meaningful. If we
choose a different basis set, the formula above allows us to express the
vector in a new basis. As mentioned above, in higher-dimensional vector
spaces a good choice of basis is important because it can greatly
simplify calculations.</p>
</div>
<div class="section" id="general-solution-of-linear-odes">
<h3>general solution of linear ODEs<a class="headerlink" href="#general-solution-of-linear-odes" title="Permalink to this headline">¶</a></h3>
<p>One very important application of vector decomposition simplifies the
solution of linear dynamical systems. Any linear system can be defined
in terms of a matrix <span class="math notranslate nohighlight">\(A\)</span>, and we saw in Chapter 4 that the solution can
be expressed in terms of its eigenvectors. The new concept is that the
set of eigenvectors of a matrix forms a basis set for the vector space,
provided the matrix is nonsingular. For instance, for a (2x2) matrix <span class="math notranslate nohighlight">\(A\)</span>
with eigenvectors <span class="math notranslate nohighlight">\(\vec v_1, \vec v_2\)</span> and eigenvalues
<span class="math notranslate nohighlight">\(\lambda_1,\lambda_2\)</span>, the simplest way to compute its effect on a
vector <span class="math notranslate nohighlight">\(\vec u\)</span> is to decompose it in the basis of the two eigenvectors:
<span class="math notranslate nohighlight">\(\vec u = c_1\vec v_1 + c_2\vec v_2 \)</span>, and then apply the matrix to the
two eigenvectors, using the linear property:
$<span class="math notranslate nohighlight">\(A\vec u = Ac_1\vec v_1 + A c_2\vec v_2 = c_1\lambda_1\vec v_1 + c_2\lambda_2\vec v_2\)</span>$</p>
<p>This gives us a simplification of a matrix multiplication to two scalar
multiplications of the two eigenvectors by the eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span>
and the coefficients <span class="math notranslate nohighlight">\(c_i\)</span>. What is needed is knowledge of the
eigenvalues and the eigenvectors, and the coefficients. We have already
seen that finding eigenvectors and eigenvalues is a difficult problem,
best left to computers. But for a given linear dynamical system, it only
needs to be done once. Then, given any initial vector <span class="math notranslate nohighlight">\(\vec u\)</span>, one can
decompose it in terms of the normalized eigenvectors, with
<span class="math notranslate nohighlight">\(c_i = \langle \vec u, \vec v_i \rangle \)</span>. Then we can obtain the
general solution for the linear dynamical systems, as a linear
combination of the eigenvectors multiplied by exponentials with rates
<span class="math notranslate nohighlight">\(\lambda_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{d \vec x}{dt} = A \vec x; \; \vec x(0) = \vec x_0 \Rightarrow \vec x(t)= \sum_i c_i e^{\lambda_i t} \vec v_i\]</div>
</div>
</div>
<div class="section" id="computational-normal-mode-calculations">
<h2>Computational: normal mode calculations<a class="headerlink" href="#computational-normal-mode-calculations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="harmonic-analysis-of-coupled-oscillators">
<h3>harmonic analysis of coupled oscillators<a class="headerlink" href="#harmonic-analysis-of-coupled-oscillators" title="Permalink to this headline">¶</a></h3>
<p><img alt="Illustration of a model of two coupled springs attached to a wall.http://en.wikipedia.org/wiki/Normal_mode[]{data-label=&quot;fig:coupled_springs&quot;}" src="fig_ch7/coupled_springs.png" />{width=”3in”}</p>
<p>In the modeling section we saw a model describing the dynamics of two
objects connected by a spring [fig:coupled_springs]. We used Hookean
potentials to describe the interactions between two masses, which
correspond to linear forces. This results in the equations of motion
that are a linear system of second-order ODEs. Before we have only seen
systems of first-order ODEs, but we can make use of linear algebra to
find the solutions. We take the matrix of the system,
$<span class="math notranslate nohighlight">\(A = \frac{k}{m}\left(\begin{array}{cc}-1 &amp; 1 \\1 &amp; -1\end{array}\right)\)</span>$</p>
<p>This matrix is known as the <em>Hessian</em> matrix, which means the matrix of
second derivatives of the potential function. The eigenvalues of this
particular Hessian matrix are 0 and <span class="math notranslate nohighlight">\(-2k/m\)</span>. With a little bit of work,
we can find the corresponding eigenvectors: <span class="math notranslate nohighlight">\(v_1 = (1,1)\)</span> for the 0
eigenvalue and <span class="math notranslate nohighlight">\(v_2 = (1,-1)\)</span> for the -2 eigenvalue. Now consider how
the solutions behave in terms of these basis vectors. If the initial
condition corresponds to the vector <span class="math notranslate nohighlight">\(v_2\)</span>, then the ODE becomes, in
matrix and vector form: $<span class="math notranslate nohighlight">\(\ddot v_2 = A v_2 = -2 k/m v_2\)</span>$</p>
<p>We know from previous examples that the solutions of this equation
(assuming <span class="math notranslate nohighlight">\(k,m &gt;0\)</span>) are purely oscillatory, with frequency
<span class="math notranslate nohighlight">\(\sqrt{2k/m}\)</span>. Note that unlike in the previous first-order ODE
examples, negative eigenvalues mean <em>oscillatory</em> behavior, rather than
exponential decay. The form of the eigenvector <span class="math notranslate nohighlight">\((1,-1)\)</span> indicates that
two masses will oscillate with opposite phases: when <span class="math notranslate nohighlight">\(x\)</span> is moving
right, <span class="math notranslate nohighlight">\(y\)</span> is moving left, and vice versa.</p>
<p>The eigenvalue of 0 is a special case. We can say that it indicated an
oscillation frequency of 0, since the second derivative equals 0, which
implies a constant velocity. This is a called a <em>translational</em> motion,
and the form of the eigenvector <span class="math notranslate nohighlight">\((1,1)\)</span> indicates that <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> move
in concert, either left or right.</p>
<p>The eigenvectors of the Hessian of a system of coupled linear
oscillators are called <em>normal modes</em>. Each one describes a <em>collective
vibrational motion</em> of a particular frequency, in which the particles
participate with relative coefficients given by the normal mode. The
corresponding eigenvalues correspond to the squared frequencies of the
collective oscillation described by the normal mode.</p>
</div>
<div class="section" id="normal-mode-calculations">
<h3>normal mode calculations<a class="headerlink" href="#normal-mode-calculations" title="Permalink to this headline">¶</a></h3>
<p>In order to perform normal mode calculations, one needs two things: the
Hessian matrix and the ability to diagonalize it (find its eigenvalues
and eigenvectors.) Let us consider that we have a system of coupled
Hookean potentials, each “spring” connecting nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> with its
own force constant <span class="math notranslate nohighlight">\(k_{ij}\)</span>. Note that Hookean potentials, so if node
<span class="math notranslate nohighlight">\(i\)</span> is connected to mode <span class="math notranslate nohighlight">\(j\)</span>, then node <span class="math notranslate nohighlight">\(j\)</span> is connected to mode <span class="math notranslate nohighlight">\(i\)</span>
with the same force constant. Then we construct the Hessian matrix as
follows:</p>
<p>obtain list of 3D coordinates for <span class="math notranslate nohighlight">\(N\)</span> nodes <span class="math notranslate nohighlight">\(X_i =(x_i,y_i, z_i)\)</span> set
cutoff distance <span class="math notranslate nohighlight">\(R\)</span> define a distance function <span class="math notranslate nohighlight">\(dist(X_i, X_j)\)</span> (returns
distance between two 3-dimensional vectors) preallocate Hessian matrix
<span class="math notranslate nohighlight">\(H\)</span> (<span class="math notranslate nohighlight">\(N\)</span> by <span class="math notranslate nohighlight">\(N\)</span>) with 0s <span class="math notranslate nohighlight">\(H[i,j] \gets -k\)</span> <span class="math notranslate nohighlight">\(H[j,i] \gets -k\)</span>
<span class="math notranslate nohighlight">\(H[j,j] \gets H[j,j] + k\)</span></p>
<p>[alg:hessian]</p>
<p>Consider three nodes connected as a linear chain, with node 1 connected
to node 2 with force constant <span class="math notranslate nohighlight">\(k_{1}\)</span>, and node 2 connected to node 3
with force constant <span class="math notranslate nohighlight">\(k_2\)</span>, the Hessian matrix is:
$<span class="math notranslate nohighlight">\(H = \left(\begin{array}{ccc}k_1 &amp; -k_1 &amp; 0 \\-k_1 &amp; k_1+k_2 &amp; -k_2 \\0 &amp; -k_2 &amp; k_2\end{array}\right)\)</span>$</p>
<p>Now that we have constructed the Hessian matrix, we need to diagonalize
it to find the normal modes. We will not describe the algorithms to find
the eigenvalues and eigenvectors of matrices, as those deserve their own
chapter, but this topic is well addressed in [&#64;press_numerical_2007].
Let us stipulate that one can use a function, such as <code class="docutils literal notranslate"><span class="pre">eig</span></code> in Matlab,
that will produce a set of eigenvectors, sorted by the magnitude of the
corresponding eigenvalues.</p>
<p>For the system of three nodes described above, let both springs have the
same force constant of 1(<span class="math notranslate nohighlight">\(k_1=k_2 = 1\)</span>.) Then the system has the
following eigenvectors and eigenvalues:
$<span class="math notranslate nohighlight">\(\vec v_1 = \left(\begin{array}{c} 1\\  1 \\1 \end{array}\right) \;  \lambda_1 = 0 ; \; \vec v_2 = \left(\begin{array}{c} -1\\  0 \\1 \end{array}\right) \;  \lambda_2 = 1 ; \; \vec v_3 = \left(\begin{array}{c} 1\\  -2 \\1 \end{array}\right) \;  \lambda_3 = 3\)</span><span class="math notranslate nohighlight">\(
This demonstrated that a linear chain of three oscillators that are not
attached to any other object has three normal modes of different
frequencies. The first mode has zero frequency, which is known as a
*rigid-body* mode, in which the entire system moves together, without
changes in relative distances. The second mode has frequency 1, and in
it the two end nodes move in opposite directions of each other. The
third mode has frequency \)</span>\sqrt{3}$, and in it the end points move in
the same direction, while the middle node moves in the opposite
direction with twice the amplitude. This analysis predicts that all
motions of the three nodes can be described in terms of the three normal
modes.</p>
</div>
</div>
<div class="section" id="synthesis-normal-mode-analysis-of-biomolecular-structures">
<h2>Synthesis: normal mode analysis of biomolecular structures<a class="headerlink" href="#synthesis-normal-mode-analysis-of-biomolecular-structures" title="Permalink to this headline">¶</a></h2>
<div class="section" id="biomolecular-structures-as-elastic-solids">
<h3>biomolecular structures as elastic solids<a class="headerlink" href="#biomolecular-structures-as-elastic-solids" title="Permalink to this headline">¶</a></h3>
<p><img alt="Cartoon depiction of the potential function of a protein, reduced totwo variables. Vertical dimension indicates relative energy level ofdifferent conformations. The sharp well in blue is the nativeconformation. http://www.btinternet.com/~martin.chaplin/protein2.html[]{data-label=&quot;fig:prot_land&quot;}" src="fig_ch7/prot_landscape.jpg" />{width=”3in”}</p>
<p>Proteins inside cells fold into exquisitely precise structures, which
enable them to perform a tremendous variety of functions, from
catalyzing biochemical reactions to binding signaling molecules. The
location of amino acids residues and all the atoms, is called the
protein’s structure. Determining the structure of a protein is
laborious, although thanks to technological advances structural
determination is less difficult than in the past. The knowledge of a
protein’s structure provided a great deal of important information to
biochemists, for instance the location of the catalytic active side.
However, it does not tell the whole story of how the protein functions.</p>
<p>The structures of proteins and other biomolecules are not static, but
instead fluctuate around the most energetically favorable conformation.
These fluctuations are cause by the jiggling of the surrounding
molecules, such as waters due to the thermal motion at the molecular
level. A protein molecule can be thought of as a system with a
potential, shaped by the interactions between all the atoms in the
molecule, and with the surrounding solvent. The variables of the system
are the positions of all the atoms, which means that the system has
thousands or tens of thousands of variables. Figure [fig:prot_land]
shows a cartoon of the potential energy function of a complex molecule.
It shows a sharp well with the preferred folded state at the minimum.
Thermal noise adds random kinetic energy to the system, causing the
conformations to jiggle in the well, and occasionally causing major
conformational changes or even unfolding.</p>
<p>Normal modes are used to study the flexibility of molecular structures.
The basic assumption is that the molecules behave as coupled harmonic
oscillators, with each atom connected to other atoms by harmonic
potentials. This assumption makes physical sense at the bottom of the
potential well, near the native conformation, where the potential must
have close to quadratic shape, and therefore the restoring forces are
nearly linear with displacement.</p>
<p><img alt="Harmonic potential model of the protein calmodulin. Green indicatesthe backbone of the molecule, maroon lines indicated harmonicinteractions betweenresidues.[]{data-label=&quot;fig:calm_gnm&quot;}" src="fig_ch7/calm_gnm.png" />{width=”4in”}</p>
<p>Various models exist for defining the connections in protein structure
between different particles, which could be atoms or amino acid
residues, or even blocks of many residues. The connections may be based
of physical chemical forces, such as chemical bonds, van der Waals
forces, and electrostatic interactions, or may be based on a simple
model where parts of the protein in proximity are assumed to interact as
if bound by a linear spring. These interactions yield a matrix
equivalent to the one we saw for two coupled oscillators, known as
Hessian matrix. Figure [fig:calm_gnm] shows the harmonic potentials
used to model the structural dynamics of the protein calmodulin.
Connections were chosen based on distance proximity between residues.
[&#64;cui_normal_2005].</p>
</div>
<div class="section" id="sorting-normal-modes-by-frequency">
<h3>sorting normal modes by frequency<a class="headerlink" href="#sorting-normal-modes-by-frequency" title="Permalink to this headline">¶</a></h3>
<p>The normal modes and the corresponding frequencies are determined by
computationally finding the eigenvectors and eigenvalues of the Hessian
matrix. For practical purposes, the most interesting modes are those
with lowest (but nonzero) frequencies, because they correspond to the
slowest and most global collective motions, as opposed to high-frequency
vibrations, which are restricted both in amplitude and in scope.
Intuitively, the lowest frequency modes correspond to the shallowest
directions in the potential energy well. Given a reasonable amount of
thermal noise, the protein structure is most likely to be deformed along
the shallow directions, instead of climbing up the steep directions.</p>
<p>The utility of normal mode analysis of biological molecules lies in
obtaining the preferred modes of flexibility from a static structure,
which allows biochemists to better understand the mechanism of the
molecular function. For instance, in studying the mechanism of opening
or closing of an enzyme binding site, normal modes can generate a
hypothesis about the intermediate conformations, and help predict which
residues play a key role. Figure [fig:calm_nma] shows the directions
of the lowest frequency mode of calmodulin, which undergoes a large
conformational change in response to binding of calcium ions. The arrows
show the extent of involvement of each amino acid residue, as well as
the direction of preferred fluctuations. To summarize, changing the
coordinate system from individual positions to collective normal modes
of motion simplifies the systems and generates predictions relevant for
understanding the function of biomolecules.</p>
<p><img alt="Lowest frequency mode predicted from normal mode analysis ofcalmodulin. Arrows indicate the direction and magnitude of flexibilityassociated with eachresidue.[]{data-label=&quot;fig:calm_nma&quot;}" src="fig_ch7/calm_nma.png" />{width=”4in”}</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dmitry Kondrashov<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>